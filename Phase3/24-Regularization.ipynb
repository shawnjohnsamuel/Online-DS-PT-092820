{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "## Why Regularize?\n",
    "\n",
    "In an attempt to fit a good model to data, we often tend to overfit. Regularization discourages overly complex models by penalizing the loss function.\n",
    "\n",
    "### The Bias-Variance Tradeoff\n",
    "\n",
    "When we did Linear Regression, we briefly talked about the Bias-Variance Tradeoff.\n",
    "\n",
    "![](http://scott.fortmann-roe.com/docs/docs/BiasVariance/biasvariance.png)\n",
    "\n",
    "![](https://miro.medium.com/max/544/1*Y-yJiR0FzMgchPA-Fm5c1Q.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**High bias** \n",
    "\n",
    " - Systematic error in predictions (i.e. the average)\n",
    " - Bias is about the strength of assumptions the model makes\n",
    " - Underfit models tend to have high bias\n",
    "\n",
    "\n",
    "**High variance**\n",
    "\n",
    " - The model is highly sensitive to changes in the data\n",
    " - Overfit models tend to have low bias and high variance\n",
    "    \n",
    "    \n",
    "![](https://gblobscdn.gitbook.com/assets%2F-LvBP1svpACTB1R1x_U4%2F-LvNWUoWieQqaGmU_gl9%2F-LvNoby-llz4QzAK15nL%2Fimage.png?alt=media&token=41720ce9-bb66-4419-9bd8-640abf1fc415)\n",
    "\n",
    " - Underfit Models fail to capture all of the information in the data\n",
    " - Overfit models fit to the noise in the data and fail to generalize\n",
    "\n",
    "\n",
    "**How would we know if our model is over or underfit?**\n",
    " - Train test split & look at the testing error\n",
    " - As model complexity increases so does the possibility for overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge and Lasso\n",
    "\n",
    "Ridge and Lasso regression are two examples of penalized estimation. Penalized estimation makes some or all of the coefficients smaller in magnitude (closer to zero). Some of the penalties have the property of performing both variable selection (setting some coefficients exactly equal to zero) and shrinking the other coefficients. \n",
    "\n",
    "In Ridge regression, the cost function is changed by adding a penalty term to the square of the magnitude of the coefficients. \n",
    "\n",
    "$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p m_j^2$$\n",
    "\n",
    "Lasso regression (Least Absolute Shrinkage and Selection Operator) is very similar to Ridge regression, except that the magnitude of the coefficients are not squared in the penalty term.\n",
    "\n",
    "$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij})-b)^2 + \\lambda \\sum_{j=1}^p \\mid m_j \\mid$$\n",
    "\n",
    "So we're penalizing large coefficients -- what are the effects/implications of that?\n",
    "\n",
    "### Standardization before Regularization\n",
    "\n",
    "An important step before using either Lasso or Ridge regularization is to first standardize your data such that it is all on the same scale. Regularization is based on the concept of penalizing larger coefficients, so **if you have features that are on different scales, some will get unfairly penalized**. A downside of standardization is that the value of the coefficients become less interpretable and must be transformed back to their original scale if you want to interpret how a one unit change in a feature impacts the target variable.\n",
    "\n",
    "**Scaler documentation:**\n",
    "\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Code! \n",
    "\n",
    "Start with a regular Linear Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/ames_train.csv') # Ames housing data\n",
    "\n",
    "# Drop sale detail columns \n",
    "df = df.drop(columns = ['Id', 'MoSold', 'YrSold', 'SaleType', 'SaleCondition'])\n",
    "\n",
    "# Create X and y\n",
    "y = df['SalePrice']\n",
    "X = df.drop(columns=['SalePrice'], axis=1)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to Clean/Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1095 entries, 1023 to 1126\n",
      "Data columns (total 75 columns):\n",
      "MSSubClass       1095 non-null int64\n",
      "MSZoning         1095 non-null object\n",
      "LotFrontage      895 non-null float64\n",
      "LotArea          1095 non-null int64\n",
      "Street           1095 non-null object\n",
      "Alley            70 non-null object\n",
      "LotShape         1095 non-null object\n",
      "LandContour      1095 non-null object\n",
      "Utilities        1095 non-null object\n",
      "LotConfig        1095 non-null object\n",
      "LandSlope        1095 non-null object\n",
      "Neighborhood     1095 non-null object\n",
      "Condition1       1095 non-null object\n",
      "Condition2       1095 non-null object\n",
      "BldgType         1095 non-null object\n",
      "HouseStyle       1095 non-null object\n",
      "OverallQual      1095 non-null int64\n",
      "OverallCond      1095 non-null int64\n",
      "YearBuilt        1095 non-null int64\n",
      "YearRemodAdd     1095 non-null int64\n",
      "RoofStyle        1095 non-null object\n",
      "RoofMatl         1095 non-null object\n",
      "Exterior1st      1095 non-null object\n",
      "Exterior2nd      1095 non-null object\n",
      "MasVnrType       1091 non-null object\n",
      "MasVnrArea       1091 non-null float64\n",
      "ExterQual        1095 non-null object\n",
      "ExterCond        1095 non-null object\n",
      "Foundation       1095 non-null object\n",
      "BsmtQual         1068 non-null object\n",
      "BsmtCond         1068 non-null object\n",
      "BsmtExposure     1068 non-null object\n",
      "BsmtFinType1     1068 non-null object\n",
      "BsmtFinSF1       1095 non-null int64\n",
      "BsmtFinType2     1068 non-null object\n",
      "BsmtFinSF2       1095 non-null int64\n",
      "BsmtUnfSF        1095 non-null int64\n",
      "TotalBsmtSF      1095 non-null int64\n",
      "Heating          1095 non-null object\n",
      "HeatingQC        1095 non-null object\n",
      "CentralAir       1095 non-null object\n",
      "Electrical       1094 non-null object\n",
      "1stFlrSF         1095 non-null int64\n",
      "2ndFlrSF         1095 non-null int64\n",
      "LowQualFinSF     1095 non-null int64\n",
      "GrLivArea        1095 non-null int64\n",
      "BsmtFullBath     1095 non-null int64\n",
      "BsmtHalfBath     1095 non-null int64\n",
      "FullBath         1095 non-null int64\n",
      "HalfBath         1095 non-null int64\n",
      "BedroomAbvGr     1095 non-null int64\n",
      "KitchenAbvGr     1095 non-null int64\n",
      "KitchenQual      1095 non-null object\n",
      "TotRmsAbvGrd     1095 non-null int64\n",
      "Functional       1095 non-null object\n",
      "Fireplaces       1095 non-null int64\n",
      "FireplaceQu      583 non-null object\n",
      "GarageType       1037 non-null object\n",
      "GarageYrBlt      1037 non-null float64\n",
      "GarageFinish     1037 non-null object\n",
      "GarageCars       1095 non-null int64\n",
      "GarageArea       1095 non-null int64\n",
      "GarageQual       1037 non-null object\n",
      "GarageCond       1037 non-null object\n",
      "PavedDrive       1095 non-null object\n",
      "WoodDeckSF       1095 non-null int64\n",
      "OpenPorchSF      1095 non-null int64\n",
      "EnclosedPorch    1095 non-null int64\n",
      "3SsnPorch        1095 non-null int64\n",
      "ScreenPorch      1095 non-null int64\n",
      "PoolArea         1095 non-null int64\n",
      "PoolQC           6 non-null object\n",
      "Fence            218 non-null object\n",
      "MiscFeature      43 non-null object\n",
      "MiscVal          1095 non-null int64\n",
      "dtypes: float64(3), int64(31), object(41)\n",
      "memory usage: 650.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Explore X_train\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's check the percentage of our training data that's null per column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop where nulls are more than 10% of column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with the continuous variables\n",
    "\n",
    "# Grab only numeric features\n",
    "\n",
    "\n",
    "# Impute missing values with 0 using SimpleImputer\n",
    "# (most columns look like they just don't have details)\n",
    "\n",
    "\n",
    "# Scale the train and test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now time for the categorical columns\n",
    "\n",
    "# Create X_cat which contains only the categorical variables\n",
    "\n",
    "# Fill missing values with the string 'missing'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Exploring column percentages\n",
    "\n",
    "# Let's remove any column where the most common value is more than 90% of that col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now drop those\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncode categorical variables\n",
    "\n",
    "\n",
    "# Convert these columns into a DataFrame \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it all back together\n",
    "\n",
    "\n",
    "# Fit the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a quick evaluation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Grab predictions and evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residuals?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Explore coefficients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate**\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's wrap up that coefficient exploration in a function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Ridge and Lasso\n",
    "\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso() # Lasso is also known as the L1 norm \n",
    "\n",
    "# Fit\n",
    "\n",
    "# Predict\n",
    "\n",
    "# Evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust HYPERPARAMETERS -- check documentation!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Lasso Coefficients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge() # Ridge is also known as the L2 norm\n",
    "\n",
    "# Fit\n",
    "\n",
    "# Predict\n",
    "\n",
    "# Evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust HYPERPARAMETERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Ridge Coefficients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Discuss\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge & Lasso: Other benefits\n",
    "\n",
    "### Ridge:\n",
    "* We can \"shrink down\" prediction variables effects instead of deleting/zeroing them\n",
    "* When you have features with high multicollinearity, the coefficients are automatically spread across them (you won't have redundancy)\n",
    "* Since includes all features it can be computationally expensive (for many variables)\n",
    "\n",
    "### Lasso:\n",
    "* When you have a lot of variables it performs feature selection for you!\n",
    "* Multicollinearity is also dealt with\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Por que no los dos??\n",
    "\n",
    "Enter ElasticNet: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
