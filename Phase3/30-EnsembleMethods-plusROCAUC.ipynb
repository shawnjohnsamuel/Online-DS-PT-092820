{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods and ROC-AUC Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial imports and grabbing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T22:54:59.601532Z",
     "start_time": "2020-10-22T22:54:59.594427Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "df = pd.read_csv('data/diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T22:55:03.107734Z",
     "start_time": "2020-10-22T22:55:03.104082Z"
    }
   },
   "outputs": [],
   "source": [
    "#create numpy arrays for predictors and target variables \n",
    "X = df.drop('Outcome',axis=1)\n",
    "y = df['Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T22:55:03.847241Z",
     "start_time": "2020-10-22T22:55:03.844457Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, A Recap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a really small simple tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's also visualize and discuss as we make changes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods: From Decision Trees to Forests\n",
    "\n",
    "Ensemble Methods take advantage of the \"wisdom of crowds\" where the average of multiple independent estimates is usually more consistently accurate than the individual estimates.\n",
    "\n",
    "### Simple Ensemble Techniques - How do we use the wisdom of the crowd? \n",
    "\n",
    "1. **Max Voting** - The max voting method is generally used for classification problems. In this technique, multiple models are used to make predictions for each data point. The predictions by each model are considered as a ‘vote’. The predictions which we get from the majority of the models are used as the final prediction.\n",
    "\n",
    "> For example, when you asked 5 of your colleagues to rate your movie (out of 5); we’ll assume three of them rated it as 4 while two of them gave it a 5. Since the majority gave a rating of 4, the final rating will be taken as 4. You can consider this as taking the mode of all the predictions.\n",
    "\n",
    "2. **Averaging** - Similar to the max voting technique, multiple predictions are made for each data point in averaging. In this method, we take an average of predictions from all the models and use it to make the final prediction. Averaging can be used for making predictions in regression problems or while calculating probabilities for classification problems. \n",
    "\n",
    "> Note - this is how sklearn's random forest classifier works)\n",
    "\n",
    "3. **Weighted Averaging** - This is an extension of the averaging method. All models are assigned different weights defining the importance of each model for prediction. \n",
    "\n",
    "> In the same previous example, if two of your colleagues are critics, while others have no prior experience in this field, then the answers by these two friends are given more importance as compared to the other people.\n",
    "\n",
    "[User Guide!](https://scikit-learn.org/stable/modules/ensemble.html#forest)\n",
    "\n",
    "### Examples of Ensembles \n",
    "\n",
    "* Bootstrap Aggregation (Bagging):\n",
    "    * Random Forests\n",
    "* Gradient Boosting algorithms:\n",
    "    * Adaboost\n",
    "    * Gradient Boosted Trees (GBM)\n",
    "    * XGBoost \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging \n",
    "\n",
    "The idea behind **bagging** is combining the results of multiple models (for instance, all decision trees) to get a generalized result. Here’s a question: If you create all the models on the same set of data and combine it, will it be useful? There is a high chance that these models will give the same result since they are getting the same input. So how can we solve this problem? One of the techniques is bootstrapping.\n",
    "\n",
    "**Bootstrapping** is a sampling technique in which we create subsets of observations from the original dataset, with replacement. The size of the subsets is the same as the size of the original set.\n",
    "\n",
    "**Bagging (or Bootstrap Aggregating)** technique uses these subsets (bags) to get a fair idea of the distribution (complete set). The size of subsets created for bagging may be less than the original set.\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/05/image20-768x289.png)\n",
    "\n",
    "\n",
    "Multiple subsets are created from the original dataset, selecting observations with replacement.\n",
    "A base model (weak model) is created on each of these subsets.\n",
    "The models run in parallel and are independent of each other.\n",
    "The final predictions are determined by combining the predictions from all the models.\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/05/Screenshot-from-2018-05-08-13-11-49-768x580.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "Random Forest is an ensemble machine learning algorithm that follows the bagging technique. It is an extension of the bagging estimator algorithm. The base estimators in random forest are decision trees. Unlike bagging meta estimator, random forest randomly selects a set of features which are used to decide the best split at each node of the decision tree.\n",
    "\n",
    "Looking at it step-by-step, this is what a random forest model does:\n",
    "\n",
    "1. Random subsets are created from the original dataset (bootstrapping).\n",
    "2. At each node in the decision tree, only a random set of features are considered to decide the best split.\n",
    "3. A decision tree model is fitted on each of the subsets.\n",
    "4. The final prediction is calculated by averaging the predictions from all decision trees.\n",
    "\n",
    "**Note:** *The decision trees in random forest can be built on a subset of data and features. Particularly, the sklearn model of random forest uses all features for decision tree and a subset of features are randomly selected for splitting at each node.*\n",
    "\n",
    "To sum up, Random forest randomly selects data points and features, and builds multiple trees (Forest) .\n",
    "\n",
    "### Some hyperparameters to tune \n",
    "\n",
    "* **n_estimators:**\n",
    "It defines the number of decision trees to be created in a random forest.\n",
    "Generally, a higher number makes the predictions stronger and more stable, but a very large number can result in higher training time.\n",
    "\n",
    "* **criterion:**\n",
    "It defines the function that is to be used for splitting.\n",
    "The function measures the quality of a split for each feature and chooses the best split.\n",
    "\n",
    "* **max_features :**\n",
    "It defines the maximum number of features allowed for the split in each decision tree.\n",
    "Increasing max features usually improve performance but a very high number can decrease the diversity of each tree.\n",
    "\n",
    "* **max_depth:**\n",
    "Random forest has multiple decision trees. This parameter defines the maximum depth of the trees.\n",
    "\n",
    "* **min_samples_split:**\n",
    "Used to define the minimum number of samples required in a leaf node before a split is attempted.\n",
    "If the number of samples is less than the required number, the node is not split.\n",
    "\n",
    "* **min_samples_leaf:** \n",
    "This defines the minimum number of samples required to be at a leaf node.\n",
    "Smaller leaf size makes the model more prone to capturing noise in train data.\n",
    "\n",
    "* **max_leaf_nodes:** \n",
    "This parameter specifies the maximum number of leaf nodes for each tree.\n",
    "The tree stops splitting when the number of leaf nodes becomes equal to the max leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T18:33:07.063027Z",
     "start_time": "2020-05-21T18:33:06.900837Z"
    }
   },
   "outputs": [],
   "source": [
    "#applying Random forest to diabetes data \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(max_depth=5, n_estimators=100, random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(rf.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(rf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T18:00:18.290056Z",
     "start_time": "2020-05-21T18:00:18.270242Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_pred = rf.predict(X_test) #predictions \n",
    "rf_score = f1_score(rf_pred, y_test) # F1 score \n",
    "rf_acc = accuracy_score(rf_pred, y_test) #Accuracy \n",
    "\n",
    "print(f'F1 Score: {rf_score:.4f}')\n",
    "print(f'Accuracy: {rf_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T18:36:24.665123Z",
     "start_time": "2020-05-21T18:36:24.652173Z"
    }
   },
   "outputs": [],
   "source": [
    "# explore feature importances\n",
    "feats = rf.feature_importances_\n",
    "feature_imps = dict(zip(X.columns, feats))\n",
    "feature_imps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T18:36:27.829370Z",
     "start_time": "2020-05-21T18:36:27.686025Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualizing feature importances\n",
    "# creating list of column names\n",
    "feat_names=list(X.columns)\n",
    "\n",
    "# Sort feature importances in descending order\n",
    "indices = np.argsort(feats)[::-1]\n",
    "\n",
    "# Rearrange feature names so they match the sorted feature importances\n",
    "names = [feat_names[i] for i in indices]\n",
    "\n",
    "# Create plot\n",
    "plt.figure()\n",
    "\n",
    "# Create plot title\n",
    "plt.title(\"Feature Importance\")\n",
    "\n",
    "# Add bars\n",
    "plt.bar(range(X_train.shape[1]), feats[indices])\n",
    "\n",
    "# Add feature names as x-axis labels\n",
    "plt.xticks(range(X_train.shape[1]), names, rotation=90)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros and Cons of Random forests \n",
    "\n",
    "**Pros:**\n",
    "* Strong performance because this is an ensemble algorithm, the model is naturally resistant to noise and variance in the data, and generally tends to perform quite well.\n",
    "\n",
    "* Interpretability: each tree in the random forest is a Glass-Box Model (meaning that the model is interpretable, allowing us to see how it arrived at a certain decision), the overall random forest is, as well!\n",
    "\n",
    "**Cons:**\n",
    "* Computational complexity: On large datasets, the runtime can be quite slow compared to other algorithms.\n",
    "\n",
    "* Memory usage: Random forests tend to have a larger memory footprint that other models. It's not uncommon to see random forests that were trained on large datasets have memory footprints in the tens, or even hundreds of MB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting \n",
    "Boosting is a sequential process, where each subsequent model attempts to correct the errors of the previous model. The succeeding models are dependent on the previous model. Let’s understand the way boosting works in the below steps.\n",
    "\n",
    "1. A subset is created from the original dataset.\n",
    "2. Initially, all data points are given equal weights.\n",
    "3. A base model is created on this subset.\n",
    "4. This model is used to make predictions on the whole dataset.\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2015/11/dd1-e1526989432375.png)\n",
    "\n",
    "5. Errors are calculated using the actual values and predicted values.\n",
    "6. The observations which are incorrectly predicted, are given higher weights.(Here, the three misclassified blue-plus points will be given higher weights)\n",
    "7. Another model is created and predictions are made on the dataset.(This model tries to correct the errors from the previous model)\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2015/11/dd2-e1526989487878.png)\n",
    "\n",
    "8. Similarly, multiple models are created, each correcting the errors of the previous model.\n",
    "9. The final model (strong learner) is the weighted mean of all the models (weak learners).\n",
    "![](https://www.analyticsvidhya.com/wp-content/uploads/2015/11/boosting10-300x205.png)\n",
    "\n",
    "Thus, the boosting algorithm combines a number of weak learners to form a strong learner. The individual models would not perform well on the entire dataset, but they work well for some part of the dataset. Thus, each model actually boosts the performance of the ensemble.\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2015/11/dd4-e1526551014644.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost \n",
    "\n",
    "[Thorough post on AdaBoost](https://www.datacamp.com/community/tutorials/adaboost-classifier-python)\n",
    "\n",
    "Adaptive boosting or **AdaBoost** is one of the simplest boosting algorithms. Usually, decision trees are used for modelling. Multiple sequential models are created, each correcting the errors from the last model. AdaBoost assigns weights to the observations which are incorrectly predicted and the subsequent model works to predict these values correctly.\n",
    "\n",
    "**Below are the steps for performing the AdaBoost algorithm:**\n",
    "\n",
    "1. Initially, all observations in the dataset are given equal weights.\n",
    "2. A model is built on a subset of data.\n",
    "3. Using this model, predictions are made on the whole dataset.\n",
    "4. Errors are calculated by comparing the predictions and actual values.\n",
    "5. While creating the next model, higher weights are given to the data points which were predicted incorrectly.\n",
    "6. Weights can be determined using the error value. For instance, higher the error more is the weight assigned to the observation.\n",
    "7. This process is repeated until the error function does not change, or the maximum limit of the number of estimators is reached.\n",
    "\n",
    "### Some hyperparameters to tune \n",
    "\n",
    "- **base_estimators:** \n",
    "\n",
    "    - It helps to specify the type of base estimator, that is, the machine learning algorithm to be used as base learner.\n",
    "    \n",
    "- **n_estimators:**\n",
    "\n",
    "    - It defines the number of base estimators.\n",
    "    -  default value is 10, but you should keep a higher value to get better performance.\n",
    "    \n",
    "- **learning_rate:** \n",
    "\n",
    "    - This parameter controls the contribution of the estimators in the final combination.\n",
    "    - There is a trade-off between learning_rate and n_estimators.\n",
    "    \n",
    "- **max_depth:**\n",
    "\n",
    "    - Defines the maximum depth of the individual estimator.\n",
    "    - Tune this parameter for best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T18:46:22.319598Z",
     "start_time": "2020-05-21T18:46:22.258037Z"
    }
   },
   "outputs": [],
   "source": [
    "#applying Adaboost \n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada = AdaBoostClassifier(random_state=1)\n",
    "ada.fit(X_train, y_train)\n",
    "ada.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting (GBM) \n",
    "\n",
    "Gradient Boosting or GBM is another ensemble machine learning algorithm that works for both regression and classification problems. GBM uses the boosting technique, combining a number of weak learners to form a strong learner. Decision trees are used as a base learner, each subsequent tree in series is built on the errors calculated by the previous tree.\n",
    "\n",
    "We will use a simple example to understand the GBM algorithm. We have to predict the age of a group of people using the below data:\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/05/image-17-768x334.png)\n",
    "\n",
    "1. The mean age is assumed to be the predicted value for all observations in the dataset.\n",
    "2. The errors are calculated using this mean prediction and actual values of age.\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/05/image-18-768x318.png)\n",
    "\n",
    "3. A tree model is created using the errors calculated above as target variable. Our objective is to find the best split to minimize the error.\n",
    "4. The predictions by this model are combined with the predictions 1.\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/06/gbm2-768x345.png)\n",
    "\n",
    "5. This value calculated above is the new prediction.\n",
    "6. New errors are calculated using this predicted value and actual value.\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/06/gbm3.png)\n",
    "\n",
    "7. Steps 2 to 6 are repeated till the maximum number of iterations is reached (or error function does not change)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T18:50:23.710134Z",
     "start_time": "2020-05-21T18:50:23.637930Z"
    }
   },
   "outputs": [],
   "source": [
    "#apply GBM to diabetes data \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbm = GradientBoostingClassifier(learning_rate=0.01, random_state=1)\n",
    "gbm.fit(X_train, y_train)\n",
    "gbm.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "XGBoost (eXtreme Gradient Boosting) is an advanced implementation of the gradient boosting algorithm. XGBoost has proved to be a highly effective ML algorithm, extensively used in machine learning competitions and hackathons. XGBoost has high predictive power and is almost 10 times faster than the other gradient boosting techniques. It also includes a variety of regularization which reduces overfitting and improves overall performance. Hence it is also known as ‘regularized boosting‘ technique.\n",
    "\n",
    "**Pros of XGBoost:** \n",
    "\n",
    "1. Regularization:\n",
    "    - Standard GBM implementation has no regularisation like XGBoost.\n",
    "    - Thus XGBoost also helps to reduce overfitting.\n",
    "    \n",
    "    \n",
    "2. Parallel Processing:\n",
    "    - XGBoost implements parallel processing and is faster than GBM .\n",
    "    - XGBoost also supports implementation on Hadoop.\n",
    "    \n",
    "    \n",
    "3. High Flexibility:\n",
    "    - XGBoost allows users to define custom optimization objectives and evaluation criteria adding a whole new dimension to the model.\n",
    "    \n",
    "    \n",
    "4. Handling Missing Values:\n",
    "    - XGBoost has an in-built routine to handle missing values.\n",
    "    \n",
    "    \n",
    "5. Tree Pruning:\n",
    "    * XGBoost makes splits up to the max_depth specified and then starts pruning the tree backwards and removes splits beyond which there is no positive gain.\n",
    "    \n",
    "    \n",
    "6. Built-in Cross-Validation:\n",
    "    * XGBoost allows a user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run.\n",
    "    \n",
    "    \n",
    "### Some hyperparameters to tune \n",
    "\n",
    "* **nthread:**\n",
    "    * This is used for parallel processing and the number of cores in the system should be entered..If you wish to run on all cores, do not input this value. The algorithm will detect it automatically.\n",
    "\n",
    "* **eta:**\n",
    "    * Analogous to learning rate in GBM.\n",
    "    * Makes the model more robust by shrinking the weights on each step.\n",
    "\n",
    "* **min_child_weight:** \n",
    "    * Defines the minimum sum of weights of all observations required in a child.\n",
    "    * Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "    \n",
    "* **max_depth:** \n",
    "    * It is used to define the maximum depth.\n",
    "    * Higher depth will allow the model to learn relations very specific to a particular sample.\n",
    "    \n",
    "* **max_leaf_nodes:** \n",
    "    * The maximum number of terminal nodes or leaves in a tree.\n",
    "    * Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "    * If this is defined, GBM will ignore max_depth.\n",
    "    \n",
    "* **gamma:** \n",
    "    * A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\n",
    "    * Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "\n",
    "* **subsample:**\n",
    "    * Same as the subsample of GBM. Denotes the fraction of observations to be randomly sampled for each tree.\n",
    "    * Lower values make the algorithm more conservative and prevent overfitting but values that are too small might lead to under-fitting.\n",
    "\n",
    "* **colsample_bytree:** \n",
    "    * It is similar to max_features in GBM.\n",
    "    * Denotes the fraction of columns to be randomly sampled for each tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T17:18:23.248505Z",
     "start_time": "2020-05-21T17:18:04.512539Z"
    }
   },
   "outputs": [],
   "source": [
    "#!conda install py-xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T18:57:08.075775Z",
     "start_time": "2020-05-21T18:57:07.998813Z"
    }
   },
   "outputs": [],
   "source": [
    "#applying boosting techniques to diabetes data \n",
    "import xgboost as xgb # note! not from sklearn\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(random_state=1, learning_rate=0.01)\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "(and now for something completely different)\n",
    "\n",
    "## ROC-AUC \n",
    "\n",
    "Aka a metric which calculates the Area Under the Curve (AUC) for the Receiver Operator Characteristic (ROC).\n",
    "\n",
    "Why is it useful? Tests not only how 'good' your model is, but also how good it is at ordering its predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Resources that Explain ROC-AUC Best:\n",
    "\n",
    "From Analytics Vidhya:\n",
    "\n",
    "> \"The Receiver Operator Characteristic (ROC) curve is an evaluation metric for binary classification problems. It is a probability curve that plots the TPR against FPR at various threshold values and essentially separates the ‘signal’ from the ‘noise’. The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve.\"\n",
    "\n",
    "| Perfect ROC Curve | More Typical ROC Curve | Bad ROC Curve (no better than guessing) |\n",
    "|---|---|---|\n",
    "|![perfect ROC curve](images/perfectAUC.webp) | ![more 'normal' ROC curve](images/midAUC.webp) | ![bad ROC curve](images/badAUC.webp) |\n",
    "\n",
    "---\n",
    "\n",
    "From [Google's Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc):\n",
    "\n",
    "> \"AUC provides an aggregate measure of performance across all possible classification thresholds. One way of interpreting AUC is **as the probability that the model ranks a random positive example more highly than a random negative example**. For example, given the following examples, which are arranged from left to right in ascending order of logistic regression predictions:\n",
    "> \n",
    "> ![auc predictions ranked](images/AUCPredictionsRanked.svg)\n",
    ">\n",
    "> \"AUC represents the probability that a random positive (green) example is positioned to the right of a random negative (red) example.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = [tree, rf, ada, gbm, xgb_model]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for model in models:\n",
    "    plot_roc_curve(model, X_test, y_test, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Resources!\n",
    "\n",
    "- [Slideshow on bagging and boosting ensemble methods](http://www2.stat.duke.edu/~rcs46/lectures_2017/08-trees/08-tree-advanced.pdf)\n",
    "- [Great short podcast on ROC-AUC](http://lineardigressions.com/episodes/2017/1/29/rock-the-roc-curve)\n",
    "    - [Plus the same podcast on Ensemble Methods](http://lineardigressions.com/episodes/2017/1/22/ensemble-algorithms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
