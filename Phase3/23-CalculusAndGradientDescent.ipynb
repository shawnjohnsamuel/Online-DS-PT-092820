{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculus and Gradient Descent\n",
    "\n",
    "![brace yourselves, math is coming](images/mathiscoming.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Up: Derivatives\n",
    "\n",
    "\"Instantaneous rate of change\"\n",
    "\n",
    "<img src=\"images/hmmm.gif\" alt=\"hmm spinning emoji gif\" width=50 align = \"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a linear function:\n",
    "\n",
    "$$\n",
    "f'(x) = \\dfrac{\\Delta y}{\\Delta x} =  \\dfrac{f(x + \\Delta x) - f(x)}{\\Delta x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requisite imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot change x,y as lines for x=1 to x=2\n",
    "\n",
    "# First, set our inputs\n",
    "\n",
    "def jog(hours):\n",
    "    '''\n",
    "    Given some amount of time, in hours, how many miles will we run?\n",
    "    Assumes our pace is 6 mph\n",
    "\n",
    "    Input: hours (time in hours)\n",
    "    Output: number of miles\n",
    "    '''\n",
    "    return 6*hours\n",
    "\n",
    "\n",
    "x1 = 1  # Input at 1 hour\n",
    "y1 = jog(x1)  # Output at 1 hour\n",
    "\n",
    "x2 = 2  # Input at 2 hours\n",
    "y2 = jog(x2)  # Output at 2 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# And now, plot\n",
    "fig, ax = plt.subplots(figsize=(7.5, 5.5))\n",
    "\n",
    "# Providing inputs to plot between x=0 and x=3.5\n",
    "x = np.linspace(0, 3.5)\n",
    "\n",
    "# Plotting our function, f(x) = 6x, between 0 and 3.5 hours\n",
    "ax.plot(x, jog(x), label=\"distance given # hours\")\n",
    "\n",
    "# Defining keyword arguments for our dashed lines\n",
    "line_kws = dict(linestyle=\"dashed\", color='darkgray')\n",
    "\n",
    "# Creating our dashed lines\n",
    "ax.hlines(y=y1, xmin=0, xmax=x1, **line_kws)\n",
    "ax.vlines(x=x1, ymin=0, ymax=y1, **line_kws)\n",
    "\n",
    "ax.hlines(y=y2, xmin=0, xmax=x2, **line_kws)\n",
    "ax.vlines(x=x2, ymin=0, ymax=y1, **line_kws)\n",
    "\n",
    "# Creating our \"rise\" portion\n",
    "ax.vlines(x=x2, ymin=y1, ymax=y2, color=\"darkorange\",\n",
    "          label=f\"y2 - y1 = {y2} - {y1} = {y2-y1}\")\n",
    "\n",
    "# Creating our \"run\" portion\n",
    "ax.hlines(y=y1, xmin=x1, xmax=x2, color=\"green\",\n",
    "          label=f\"x2 - x1 = {x2} - {x1}  = {x2-x1}\")\n",
    "\n",
    "ax.legend(loc='upper left', fontsize='large')\n",
    "ax.set_ylabel(\"distance in miles\")\n",
    "ax.set_xlabel(\"number of hours\")\n",
    "plt.title(\"distance over time, given a 6mph pace\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So... derivative?\n",
    "\n",
    "What's the derivative for this function, given that the function is $f(x) = 6x$ ?\n",
    "\n",
    "- \n",
    "\n",
    "\n",
    "But wait, and this is important: What type of function is this?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting something a bit more complicated...\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# Providing inputs to plot between x=-6 and x=10\n",
    "x_values = np.linspace(-6, 10)\n",
    "# Defining our y values with list comprehension \n",
    "function_values = [2*x**2-8*x for x in x_values]\n",
    "\n",
    "# Plotting our axes at x=0 and y=0\n",
    "plt.axhline(y=0, color='lightgrey', )\n",
    "plt.axvline(x=0, color='lightgrey')\n",
    "\n",
    "# The plot!\n",
    "plt.plot(x_values, function_values, label = \"f(x) = 2x^2 - 8x\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the derivative for this function, given that the function is $f(x) = 2x^2 - 8x$ ?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter Machine Learning\n",
    "\n",
    "This is a good time to think for a second about what it means to build models. Ultimately we're saying that, given some number of inputs (x, or features in our datasets) we can predict the outcome (y, or our target variable).\n",
    "\n",
    "When we first started with linear regression, we were trying to find a **line of best fit** (and we'll get to that \"best fit\" part in a second) which captures some way of taking in a single variable (x) and transforming it (by maybe multiplying by a slope and maybe adding some constant) to find a continuous output (y).\n",
    "\n",
    "> **Example:** if we believe the **miles per gallon rate** of a car is just a function of its **horsepower**, we would try to find the rate at which the horsepower then changes the MPG, plus some constant if the MPG isn't exactly zero when the horsepower is zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the data from 'auto-mpg.csv'\n",
    "mpg_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first try this out ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plotting the actual relationship between horsepower and mpg\n",
    "sns.scatterplot(x='horsepower', y='mpg', data=mpg_df)\n",
    "\n",
    "plt.xticks(ticks=range(0, 250, 50))\n",
    "plt.yticks(ticks=range(0, 55, 5))\n",
    "\n",
    "plt.title('Relationship Between Horsepower and MPG')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What function captures this relationship?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, with machine learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a linear regression model using just the horsepower column\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# y = mx + b\n",
    "print(f'Slope: {m}')\n",
    "print(f'Intercept: {b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the prediction for a row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's plot the line over the actual values\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plotting the actual relationship between horsepower and mpg\n",
    "sns.scatterplot(x='horsepower', y='mpg', data=mpg_df)\n",
    "\n",
    "# Now plotting the line of best fit found by our model\n",
    "axes = plt.gca()\n",
    "x_vals = range(0, 250)\n",
    "y_vals = b + m * x_vals\n",
    "# Would be the same as model.predict(np.array(x_vals).reshape(-1,1))\n",
    "plt.plot(x_vals, y_vals, '--', color='orange')\n",
    "\n",
    "plt.title('Relationship Between Horsepower and MPG')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph shows the approximate best fit line for the relationship between `horsepower` and `mpg` in our data, which we found through sklearn's function. But what if we wanted to test how it changes if we change that slope?\n",
    "\n",
    "As you can imagine, as we test out different slopes, keeping the y-intercept constant, we can see how that affects our error - our Residual Sum of Squares (RSS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Quick Aside on Cost Functions\n",
    "\n",
    "> A cost function is a function that calculates the error of our models predictions vs ground truth.\n",
    ">\n",
    "> \"Cost function\" = \"Loss function\" = \"Error\"\n",
    "    \n",
    "#### Residual Sum of Squares\n",
    "\n",
    "$ \\large RSS = \\sum_{i=1}^n(actual - expected)^2 = \\sum_{i=1}^n(y_i - \\hat{y})^2 $\n",
    "\n",
    "#### Mean Squared Error\n",
    "\n",
    "$ \\large MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2$\n",
    "\n",
    "- Note that MSE is just RSS divided by the number of data points. So its the *mean* of the residual sum of squares (AKA squared error)\n",
    "\n",
    "#### Root Mean Squared Error\n",
    "\n",
    "$ \\large  RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2}$\n",
    "\n",
    "- Note that RMSE is just the square root of MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RSS(x_values, y_values, m, b):\n",
    "    y_pred = (b + m*x_values)\n",
    "    return np.sum(np.square(y_pred - y_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = mpg_df['horsepower'] # Grabbing x as a series this time\n",
    "y = mpg_df['mpg'] # And our target\n",
    "\n",
    "results = {}\n",
    "# Testing 20 slope options between -0.5 and 0.5\n",
    "for slope in np.linspace(-0.5, 0.5, 20):\n",
    "    results[slope] = RSS(x, y, slope, 39.93) # remember, holding b constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in results.items():\n",
    "    print(f\"Slope: {k}: {v:,.2f}\") # using :,.2f to round and add commas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below graph zooms in to show how the model chose that exact slope for the line of best fit, by showing the residual sum of squares (RSS) as you change the slope of that line.\n",
    "\n",
    "![Slope-RSS relationship image](images/slope-rss-relationship.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above curve is just for ONE coefficient, the slope, while holding the y-intercept constant. Already with simple linear regression we have to start using gradients, from partial derivatives, in order to optimize both parameters at the same time.\n",
    "\n",
    "<img alt=\"gradient descent image from Section 23 - Gradient Descent: Step Sizes\" src=\"images/gradientdescent.png\" width=400>\n",
    "\n",
    "And then we made linear regression more complicated by realizing that hey, more than one variable could have an impact on our target.\n",
    "\n",
    "> Back to the example: If horsepower alone cannot be used to predict the miles per gallon rate of a car, maybe the horsepower feet plus the number of cylinders plus the weight of the car actually does predict the MPG. Each of those three separate variables will have their own way of changing the MPG, that rate of change, and in the end you might have some constant term too\n",
    "\n",
    "All of these coefficients in linear regression are really the variables we're trying to adjust and optimize based on our training data, so we let statsmodels or sklearn compute gradients and find optimal parameters to best capture the relationship you have in your data.\n",
    "\n",
    "Ultimately, when you are modeling data, I think it's easy to lose track of the fact that all you're doing is saying hey, given some inputs, I can figure out the relationship (captured in some probably increasingly complicated function) that can then be used to estimate an output in a way I find useful. \n",
    "\n",
    "Now, back to that sense of \"best fit\" - where, in pure math and when you're taught some of this calculus/derivative stuff in school, you start with a function that's provided in the textbook and asked to do stuff with it - instead, here we're saying I have these number of inputs, and some output I know for my training data, so let's find those rates of change and that constant that then captures the actual function of the line. \n",
    "\n",
    "That's where the gradients come in - as we add more and more coefficients in our linear regression model to optimize, it becomes too computationally expensive to do all of these steps every single time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Size\n",
    "\n",
    "The truth is, we can't simply use the derivative to find the minimum. Using that approach will be impossible in many scenarios as our models become more complicated.\n",
    "\n",
    "We also can't alter all of the variables of our regression line across all points and calculate the result.  It will take too much time, as we have more variables to alter. \n",
    "\n",
    "But we can make changes, see what happens, and make more changes - all in an educated, math-informed way.\n",
    "\n",
    "> Let's call each of these changes a **step**, and the size of the change our **step size**. \n",
    "\n",
    "1. A small learning rate requires many updates before reaching the minimum \n",
    "2. The optimal learning rate quickly converges to the minimum point \n",
    "3. A learning rate that is too large leads to divergent behavior: you may bounce around the minimum!  \n",
    "\n",
    "True Gradient Descent doesn't just try a fixed number of evenly spaced values - it uses the size of the slope to indicated **how much** the parameter should change (the **step size**).\n",
    "\n",
    "We use a parameter called the **learning rate** to control how rapidly we update the parameter.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-lp-gradient-descent/master/images/learning_rates.png\" width=70%>\n",
    "\n",
    "Our new task is to find step sizes that bring us to the best RSS quickly without overshooting the mark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "![gradient descent in 3d gif from Andrew Ng](https://thumbs.gfycat.com/AngryInconsequentialDiplodocus-size_restricted.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips for Gradient Descent\n",
    "\n",
    "A few of the tips mentioned in [Machine Learning Mastery's tutorial on Gradient Descent for Machine Learning](https://machinelearningmastery.com/gradient-descent-for-machine-learning/)\n",
    "\n",
    "- **Plot Cost versus Time:** Collect and plot the cost values calculated by the algorithm each iteration, when we get to models that give you their iterations. The expectation for a well performing gradient descent run is a decrease in cost each iteration. If it does not decrease, try reducing your learning rate.\n",
    "- **Learning Rate:** The learning rate value is a small real value such as 0.1, 0.001 or 0.0001. Try different values for your problem and see which works best.\n",
    "- **Rescale Inputs:** The algorithm will reach the minimum cost faster if the shape of the cost function is not skewed and distorted. You can achieved this by rescaling all of the input variables (X) to the same range, such as [0, 1] or [-1, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "There are so many!\n",
    "\n",
    "#### Calculus:\n",
    "\n",
    "- To build intuition about concepts in calculus, check out [3 Blue 1 Brown's Essence of Calculus Youtube Playlist](https://www.youtube.com/watch?v=WUvTyaaNkzM&list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr) - 12 videos that start off by explaining derivatives\n",
    "- The same guy who runs the 3 Blue 1 Brown account also does some videos for Khan Academy, including the [Partial Derivatives videos from the Multivariable Calculus course](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives)\n",
    "\n",
    "#### Gradient Descent:\n",
    "\n",
    "- Andrew Ng was a famous name in machine learning before he created the ultra-popular [Machine Learning Coursera course](https://www.coursera.org/learn/machine-learning), which has a great explanation of gradient descent\n",
    "\n",
    "    - [This blog post by Chris McCormick](https://mccormickml.com/2014/03/04/gradient-descent-derivation/) breaks out some of the explanation from the course, specifically trying to derive the math\n",
    "    \n",
    "- As referenced above, [Machine Learning Mastery has a tutorial on Gradient Descent for Machine Learning](https://machinelearningmastery.com/gradient-descent-for-machine-learning/), walks through gradient descent with surprisingly little math"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
