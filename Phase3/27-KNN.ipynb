{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# K Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors is a **non-parametric**, **lazy** learning algorithm. \n",
    "\n",
    "What does this mean?\n",
    "\n",
    "- **Non-parametric models** make no underlying assumptions about the distribution of data\n",
    "- **Lazy learners** (or **instance-based** learning methods) simply store the training examples and postpone the generalization (building a model) until a new instance must be classified or prediction made\n",
    "    - In other words, no training is necessary! This makes training super fast but testing is slower and costly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### What should the grey point be?\n",
    "\n",
    "![example scenario](images/scenario.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## KNN has the following basic steps:\n",
    "\n",
    "![knn process](images/knn-process.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The algorithm can be summarized as:**\n",
    "\n",
    "1. A positive integer `k` is specified\n",
    "2. We select the `k` entries in our training data which are closest to the new sample\n",
    "3. We find the most common classification of these entries (voting)\n",
    "4. This is the classification we give to the new sample\n",
    "\n",
    "**A few other features of KNN:**\n",
    "\n",
    "* KNN stores the entire training dataset which it uses as its representation\n",
    "* KNN does not learn any model parameters\n",
    "* KNN makes predictions 'just-in-time' by calculating the similarity between an input sample and each training instance\n",
    "\n",
    "**Note:** KNN performs better with a low number of features. The more features you have the more data you need. You increase the dimensions everytime you add another feature. Increase in dimension also leads to the problem of overfitting. To avoid overfitting, the needed data will need to grow exponentially as you increase the number of dimensions. This problem of higher dimension is known as the Curse of Dimensionality.\n",
    "\n",
    "Resource on the Curse of Dimensionality for classification: https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Voting\n",
    "\n",
    "How to break ties:\n",
    "\n",
    "1. When doing a binary classification, often use a odd `k` to avoid ties.\n",
    "2. Multiple approaches for multi-class problems:\n",
    "    - Reduce the `k` by 1 to see who wins.\n",
    "    - Weight the votes based on the distance of the neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example training data\n",
    "\n",
    "This example uses a multi-class problem and each color represents a different class.\n",
    "\n",
    "![data overview](images/04_knn_dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### KNN classification map (K=1)\n",
    "\n",
    "![1NN classification map](images/04_1nn_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### KNN classification map (K=5)\n",
    "\n",
    "![5NN classification map](images/04_5nn_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distance Metrics\n",
    "\n",
    "### What are distance metrics? \n",
    "\n",
    "Formulas that can be used to measure the distance between our variables \n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*FlMiuoENrq52tMV4S6LSZg.png)\n",
    "\n",
    "* Euclidean and Manhattan distance are typically best for continuous variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementing the KNN Classifier with SKlearn\n",
    "\n",
    "For reference: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[Following this analysis for part of this section](https://www.kaggle.com/shrutimechlearn/step-by-step-diabetes-classification-knn-detailed)\n",
    "\n",
    "Can the minimum value of all of these columns be zero (0)?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with proper nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use distributions to decide imputation strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes[zero_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class imbalance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y\n",
    "\n",
    "X = None\n",
    "y = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need more imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Train test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to impute then scale data - KNN is sensitive to scale!\n",
    "imputer = None\n",
    "scaler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# And now, modeling!\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Predictions for the testing set\n",
    "y_pred_class = knn.predict(X_test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Metrics\n",
    "from sklearn import metrics\n",
    "print(f'Accuracy: {metrics.accuracy_score(y_test, y_pred_class):.4f}')\n",
    "print(f'F1: {metrics.f1_score(y_test, y_pred_class):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.plot_confusion_matrix(knn, X_test_processed, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using a different value for K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# instantiate the model (using some other value)\n",
    "\n",
    "# fit the model with data\n",
    "\n",
    "# make class predictions for the testing set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Accuracy: {metrics.accuracy_score(y_test, y_pred_class):.4f}')\n",
    "print(f'F1: {metrics.f1_score(y_test, y_pred_class):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.plot_confusion_matrix(knn, X_test_processed, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Search for an optimal value of `k` for KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = list(range(1, 20, 2)) # testing odd ks between 1 and 19\n",
    "k_train_scores = []\n",
    "k_test_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    #Setup a knn classifier with k neighbors\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    \n",
    "    #Fit the model\n",
    "    knn.fit(X_train_processed, y_train)\n",
    "    \n",
    "    train_preds = knn.predict(X_train_processed)\n",
    "    test_preds = knn.predict(X_test_processed)\n",
    "\n",
    "    #Compute f1score on the training set\n",
    "    k_train_scores.append(metrics.f1_score(y_train, train_preds))  \n",
    "    #Compute f1score on the test set\n",
    "    k_test_scores.append(metrics.f1_score(y_test, test_preds))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate plot\n",
    "plt.figure(figsize=(12, 6))  \n",
    "plt.plot(k_range, k_test_scores, label='Testing F1 Score')\n",
    "plt.plot(k_range, k_train_scores, label='Training F1 Score')\n",
    "plt.title('F1 score by K Value')\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.xticks(k_range)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# zoom in just on test scores\n",
    "plt.figure(figsize=(12, 6))  \n",
    "plt.plot(k_range, k_test_scores, color='red', marker='o',  \n",
    "         markerfacecolor='blue')\n",
    "plt.title('F1 score by K Value - just the test set')  \n",
    "plt.xlabel('Number of Neighbords')  \n",
    "plt.ylabel('F1 Score') \n",
    "plt.xticks(k_range)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What value of K performs best on our Test data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here we use F score, what other metrics could we use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How do you think K size relates to our concepts of bias and variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![bias variance notes](images/K-NN_Neighborhood_Size_print.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review - Pros and Cons of KNNs \n",
    "\n",
    "**Pros:**\n",
    "- No assumptions about data — useful, for example, for nonlinear data\n",
    "- Simple algorithm — to explain and understand/interpret\n",
    "- High accuracy (relatively) — it is pretty high but not competitive in comparison to better supervised learning models\n",
    "- Versatile — useful for classification or regression\n",
    "\n",
    "**Cons:**\n",
    "- Computationally expensive — because the algorithm stores all of the training data\n",
    "- High memory requirement\n",
    "- Stores all (or almost all) of the training data\n",
    "- Prediction stage might be slow (with big N)\n",
    "- Sensitive to irrelevant features and the scale of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resources\n",
    "\n",
    "- [Nearest Neighbors](http://scikit-learn.org/stable/modules/neighbors.html) (user guide), [KNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) (class documentation)\n",
    "\n",
    "- [Videos from An Introduction to Statistical Learning](http://www.dataschool.io/15-hours-of-expert-machine-learning-videos/)\n",
    "    - Classification Problems and K-Nearest Neighbors (Chapter 2)\n",
    "    - Introduction to Classification (Chapter 4)\n",
    "    - Logistic Regression and Maximum Likelihood (Chapter 4)\n",
    "    \n",
    "- [Curse of Dimensionality](https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
