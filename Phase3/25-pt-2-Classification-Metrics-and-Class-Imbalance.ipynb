{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Metrics and Class Imbalance\n",
    "## (Featuring Some Logistic Regression Practice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start off with a page from [Google's Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative) and talk about a classic classification problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Boy Who Cried 'Wolf'\n",
    "\n",
    "![adorable wolf image from instagram user fablefire: https://www.instagram.com/p/CCGgVLGFneE/](images/awoo.png)\n",
    "\n",
    "In the old fable about the boy who cried 'wolf' there are two possible outcomes: \n",
    "\n",
    "- **No Wolf** - negative outcome, or 0\n",
    "- **Wolf** - positive outcome, or 1\n",
    "\n",
    "(I know, having a wolf arrive is not \"positive\" - but it is what we're trying to predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you think of this as a model, where the shepherd is predicting whether or not a wolf will threaten the flock of sheep:\n",
    "\n",
    "![outcome description for wolf scenarios as a confusion matrix](images/wolf_confusion_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what does that look like with data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of the imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix # New to version 0.22.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/titanic.csv', index_col = \"PassengerId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Survived'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-less Baseline\n",
    "\n",
    "First of all, I want to see how well the model will do if it predicts the majority class. In other words, if the model only predicts that no one survives, what percentage of the time would it be right? \n",
    "\n",
    "How do we do this? Find the number of passengers who didn't survive, divide by the total number of passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find percent who did not survive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Another way to do this - the mean of this column gives the percentage of\n",
    "# passengers who DID survive because it's a 0/1 binary - find the inverse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = df['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_baseline = [0] * num_passengers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_actual, y_pred_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A confusion matrix\n",
    "confusion_matrix(y_actual, y_pred_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, prettier: \n",
    "\n",
    "<img alt=\"table view with colors to show results of modelless baseline\" src=\"images/titanic_modelless_baseline_cm.png\" height=200 width=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix &rarr; Classification Metrics\n",
    "\n",
    "That block above, where we hashed out true negatives / true positives / false negatives / false positives, is called a **Confusion Matrix** - a summary of how well a classification model was able to predict each class. Across one axis you have the _predicted_ labels, and across the other axis you have the _actual_ labels, and thus you're able to clearly see the breakdown of where a model is making mistakes - and, more importantly, what kinds of mistakes your model is making.\n",
    "\n",
    "So - how does a confusion matrix translate into classification metrics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Interpretation\n",
    "\n",
    "\n",
    "<img alt=\"confusion matrix interpretation with metrics\" src=\"images/confusion_matrix_interpretation.png\" height=600 width=600>\n",
    "\n",
    "Note that I've highlighted the most often used metrics in blue above. \n",
    "\n",
    "In other words, those metrics are:\n",
    "\n",
    "- Accuracy: All True Predictions / All Predictions\n",
    "\n",
    "- Precision score: TP / All Predicted Positives\n",
    "\n",
    "- Recall or Sensitivity: TP / All Actual Positives \n",
    "\n",
    "There's one more score that's often referenced which balances precision and recall - it's called an [**F1 Score**](https://en.wikipedia.org/wiki/F1_score).\n",
    "\n",
    "$$ \\text{F1 Score} = 2 * \\frac{ precision * recall}{precision + recall} $$\n",
    "\n",
    "\n",
    "**Let's Discuss**: Why might we care more about precision than recall, or vice versa?\n",
    "\n",
    "- \n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "Moving on - luckily SKLearn will of course calculate these scores for us. You can see all of their classification metrics [here](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics).\n",
    "\n",
    "(Yes, there are more metrics - including the ROC-AUC score introduced in the curriculum - that we have not discussed so far. Stay tuned - we'll discuss that metric specifically in a future study group session!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Practice\n",
    "\n",
    "![monty python, 'nobody expects the logistic regression](images/nobodyexpectslogreg.jpg)\n",
    "\n",
    "In general, I'll be following the imputation/scaling strategy outlined [here](https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html) (just in a more laid-out way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to use the columns: `Age`, `Fare`, `Sex`, and `Pclass` - a combination of types, also `Age` has null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's encode our categorical column, Sex\n",
    "\n",
    "\n",
    "# Add the column to the dataframe, but now encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining my X and y\n",
    "\n",
    "X = None\n",
    "y = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And then doing a train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Age, I need to impute nulls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's scale our data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, fitting our model and grabbing our training and testing predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Score?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the confusion matrix using SKLearn \n",
    "plot_confusion_matrix(logreg, X_test_scaled, y_test,\n",
    "                      cmap=plt.cm.Blues, # Changing the color scheme\n",
    "                      values_format=\".3g\") # Formatting the numbers properly\n",
    "plt.grid(False) # This just removes an annoying grid that shows up by default\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Printing the metrics nicely\n",
    "metrics = {\"Accuracy\": accuracy_score,\n",
    "           \"Recall\": recall_score,\n",
    "           \"Precision\": precision_score,\n",
    "           \"F1-Score\": f1_score}\n",
    "\n",
    "for name, metric in metrics.items():\n",
    "    print(f\"{name}:\"); print(\"=\"*len(name))\n",
    "    print(f\"TRAIN: {metric(y_train, train_preds):.4f}\")\n",
    "    print(f\"TEST: {metric(y_test, test_preds):.4f}\")\n",
    "    print(\"*\" * 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation:\n",
    "\n",
    "So, how'd we do? Specifically, how'd we do compared to our model-less baseline?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "What can we potentially do to improve this model?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does a class imbalance look like?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we care?\n",
    "\n",
    "Think about it - you're asking a computer, which has NO idea what you're talking about or how to identify anything in any way other than how you tell it to identify things, to look at something completely new and categorize it. If you feed it 1000 emails, 950 of which are 'not spam' and 50 of which are 'spam,' and ask it to identify which are 'not spam,' it can just label everything as 'not spam' and be 95% correct! Not bad!\n",
    "\n",
    "And yet... that doesn't do what you want at all. You want your model to learn the characteristics of 'spam' emails and actually identify the parts of it which are reliable predictors for 'spam' in general, something the computer is increasingly incentivized not to do as the majority in your datasets gets larger compared to the minority. If your target is really imbalanced, your model will have to work increasingly harder in order to do better than the model-less baseline of just predicting the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can we do about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Under-Sampling\n",
    "\n",
    "Basically, take a sample to reduce the majority class to be the same size as the minority class.\n",
    "\n",
    "Example:\n",
    "```\n",
    "minority = df.loc[df[\"category\"] == \"minority\"]\n",
    "majority = df.loc[df[\"category\"] == \"majority\"].sample(n=len(minority))\n",
    "```\n",
    "\n",
    "Problems?\n",
    "\n",
    "- Losing a lot of observations (in the 50 spam vs 950 not-spam example, we'd lose 900 rows!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Over-Sampling\n",
    "\n",
    "The opposite - keep resampling from our minority class until it's the same size as the majority class.\n",
    "\n",
    "Example:\n",
    "```\n",
    "majority = df.loc[df[\"category\"] == \"majority\"]\n",
    "minority = df.loc[df[\"category\"] == \"minority\"].sample(n=len(majority), replace=True)\n",
    "```\n",
    "\n",
    "Problems?\n",
    "\n",
    "- Will over-fit to the minority class, since it'll see the same minority examples over and over again (in the same 50 spam vs 950 not-spam example, we'd likely repeat each of the rows in the minority class 19 times!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split The Difference\n",
    "\n",
    "Basically, balance Under and Over sampling so that you do a bit of both - might be better than relying on just one of the above strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthetic Data Creation - ADASYN and SMOTE\n",
    "\n",
    "The **Synthetic Minority Oversampling Technique (SMOTE)** conducts cluster-based over-sampling. SMOTE works by finding all the instances of the minority category within the observations, drawing lines between those instances, and then creating new observations along those lines.\n",
    "\n",
    "![SMOTE visualized](images/SMOTE_R_visualisation_3.png)\n",
    "\n",
    "Image source is a great explainer on SMOTE (but uses R for the examples): https://rikunert.com/SMOTE_explained\n",
    "\n",
    "This is better than simply using a random over-sample, yet not only are these synthetic samples not real data but also these samples are based on your existing minority. So, those new, synthetic samples can still result in over-fitting, since they're made from our original minority category. An additional pitfall you might run into is if one of your minority category is an outlier - you'll have new data that creates synthetic data based on the line between that outlier and another point in your minority, and maybe that new synthetic data point is also an outlier.\n",
    "\n",
    "Another way to create synthetic data to over-sample our minority category is the **Adaptive Synthetic approach, ADASYN**. ADASYN works similarly to SMOTE, but it focuses on the points in the minority cluster which are the closest to the majority cluster, aka the ones that are most likely to be confused, and focuses on those. It tries to help out your model by focusing on where it might get confused, where 'spam' and 'not spam' are the closest, and making more data in your 'spam' minority category there.\n",
    "\n",
    "\n",
    "Check out the library [imblearn](https://imbalanced-learn.org/stable/) for implementation of these!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, of course, sklearn has some methods to handle imbalanced datasets built right into some models - including logistic regression!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try a model with an adjusted hyperparameter...\n",
    "logreg_b = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, fitting our model and grabbing our training and testing predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the confusion matrix using SKLearn\n",
    "plot_confusion_matrix(logreg_b, X_test_scaled, y_test,\n",
    "                      cmap=plt.cm.Blues, # Changing the color scheme\n",
    "                      values_format=\".3g\") # Formatting the numbers properly\n",
    "plt.grid(False) # This just removes an annoying grid that shows up by default\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the metrics nicely\n",
    "metrics = {\"Accuracy\": accuracy_score,\n",
    "           \"Recall\": recall_score,\n",
    "           \"Precision\": precision_score,\n",
    "           \"F1-Score\": f1_score}\n",
    "\n",
    "for name, metric in metrics.items():\n",
    "    print(f\"{name}:\"); print(\"=\"*len(name))\n",
    "    print(f\"TRAIN: {metric(y_train, train_preds):.4f}\")\n",
    "    print(f\"TEST: {metric(y_test, test_preds):.4f}\")\n",
    "    print(\"*\" * 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neat Tricks (if we have time)\n",
    "\n",
    "Let's try the code directly from [imputation/scaling strategy example I referenced earlier](https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html), which uses some neat tricks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['Age', 'Fare']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_features = ['Sex', 'Pclass']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression())])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns='Survived'), y, test_size=0.2)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the confusion matrix using SKLearn\n",
    "plot_confusion_matrix(clf, X_test, y_test,\n",
    "                      cmap=plt.cm.Blues, # Changing the color scheme\n",
    "                      values_format=\".3g\") # Formatting the numbers properly\n",
    "plt.grid(False) # This just removes an annoying grid that shows up by default\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = clf.predict(X_train)\n",
    "y_test_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, metric in metrics.items():\n",
    "    print(f\"{name}:\"); print(\"=\"*len(name))\n",
    "    print(f\"TRAIN: {metric(y_train, y_train_pred):.4f}\")\n",
    "    print(f\"TEST: {metric(y_test, y_test_pred):.4f}\")\n",
    "    print(\"*\" * 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources:\n",
    "\n",
    "- [SMOTE Explained for Noobs](https://rikunert.com/SMOTE_explained) (the R tutorial I linked earlier)\n",
    "- [Resampling Strategies for Imbalanced Datasets](https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets)\n",
    "- Machine Learning Mastery: [8 Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/)\n",
    "- [Handling Imbalanced Datasets in Deep Learning](https://towardsdatascience.com/handling-imbalanced-datasets-in-deep-learning-f48407a0e758)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
