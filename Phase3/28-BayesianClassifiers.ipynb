{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we are using an API to gather articles from a news website and grabbing phrases from two different types of articles: on **sports** and on **politics**.\n",
    "\n",
    "Is there a way we can use machine learning to help us label the articles quickly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sports = ['the match was close',\n",
    "          'the coaches agreed on strategy',\n",
    "          'played in a sold out stadium']\n",
    "\n",
    "politics = ['world leaders met last week',\n",
    "            'the election was close',\n",
    "            'the officials agreed on a compromise']\n",
    "\n",
    "test_statement = 'world leaders agreed to fund the stadium'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing Back Bayes\n",
    "\n",
    "> \"Naive Bayes classifiers are linear classifiers that are known for being **simple yet very efficient**. The probabilistic model of naive Bayes classifiers is based on Bayes’ theorem, and the adjective naive comes from the assumption that the features in a dataset are **mutually independent**. In practice, the independence assumption is often violated, but naive Bayes classifiers **still tend to perform very well** under this unrealistic assumption. Especially for small sample sizes, naive Bayes classifiers can outperform the more powerful alternatives.\"\n",
    "\n",
    "[Source: Sebasitian Raschka: Naive Bayes and Text Classification](https://sebastianraschka.com/Articles/2014_naive_bayes_1.html) (emphasis is mine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revisiting the theorem itself:\n",
    "\n",
    "![bayes theorem](images/bayes_theorem.svg)\n",
    "\n",
    "#### AKA\n",
    "\n",
    "![breaking down the function behind naive bayes](images/naive_bayes_icon.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Another way of looking at it\n",
    "\n",
    "![further breakdown of the pieces of naive bayes](images/another_one.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, in the context of our problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## $ P(politics | document) = \\frac{P(document|politics)P(politics)}{P(document)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need to calculate each piece in turn..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How should we calculate $ P(politics) $ ?\n",
    "\n",
    "This is essentially the distribution of the probability of either type of article. We have three of each type of article in our current set of example documents, therefore, we assume that there is an equal probability of either type.\n",
    "\n",
    "\n",
    "## $ P(politics) = \\frac{\\# politics\\ documents}{\\# all\\ documents} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can check this, though\n",
    "# going back to our intro example data...\n",
    "p_politics = len(politics)/(len(politics) + len(sports))\n",
    "p_politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_sports = len(sports)/(len(politics) + len(sports))\n",
    "p_sports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How should we calculate $ P(document | politics) $ ?\n",
    "\n",
    "We need to break the phrases down into individual words - with the hope that these words actually tell us more, since we likely have never seen this exact document before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ P(phrase | politics) = \\prod_{i=1}^{d} P(word_{i} | politics) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ P(word_{i} | politics) = \\frac{\\#\\ of\\ word_{i}\\ in\\ politics\\ docs} {\\#\\ of\\ total\\ words\\ in\\ politics\\ docs} $\n",
    "\n",
    "#### Can you foresee any issues with this?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enter: Laplace Smoothing\n",
    "\n",
    "### $ P(word_{i} | politics) = \\frac{\\#\\ of\\ word_{i}\\ in\\ politics\\ docs + \\alpha} {\\#\\ of\\ total\\ words\\ in\\ politics\\ docs + \\alpha d} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This correction process is called Laplace Smoothing:\n",
    "\n",
    "- d : number of features (in this instance total number of vocabulary words)\n",
    "- $\\alpha$ can be any number greater than 0 (it is usually 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How should we calculate $ P(document) $ ?\n",
    "\n",
    "- well... we don't have to, because the P(document) doesn't change whether we're looking at sports or politics, we're just going to compare the numerator of these to see which is bigger between sports or politics!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So why is this 'naive' ?\n",
    "\n",
    "> \"Naive Bayes (NB) is ‘naive’ because it makes the assumption that features of a measurement are independent of each other. This is naive because it is (almost) never true.\"\n",
    "\n",
    "[Source - 'What's So Naive About Naive Bayes?', a Towards Data Science blog post all about this](https://towardsdatascience.com/whats-so-naive-about-naive-bayes-58166a6a9eba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's calculate this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sports)\n",
    "print(politics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| word       | frequency in politics | frequency in sports |\n",
    "| ---------- | --------------------- | ------------------- |\n",
    "| the        |  2                    | 2                   |\n",
    "| match      |  0                    | 1                   |\n",
    "| was        |  1                    | 1                   |\n",
    "| close      |  1                    | 1                   |\n",
    "| coaches    |  0                    | 1                   |\n",
    "| agreed     |  1                    | 1                   |\n",
    "| on         |  1                    | 1                   |\n",
    "| strategy   |  0                    | 1                   |\n",
    "| played     |  0                    | 1                   |\n",
    "| in         |  0                    | 1                   |\n",
    "| a          |  1                    | 1                   |\n",
    "| sold       |  0                    | 1                   |\n",
    "| out        |  0                    | 1                   |\n",
    "| stadium    |  0                    | 1                   |\n",
    "| world      |  1                    | 0                   |\n",
    "| leaders    |  1                    | 0                   |\n",
    "| met        |  1                    | 0                   |\n",
    "| last       |  1                    | 0                   |\n",
    "| week       |  1                    | 0                   |\n",
    "| election   |  1                    | 0                   |\n",
    "| officials  |  1                    | 0                   |\n",
    "| compromise |  1                    | 0                   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Test sentence: 'world leaders agreed to fund the stadium'\n",
    "\n",
    "| word    | $ P( word | politics) $                | $ P( word | sports) $                  |\n",
    "| ------- | -------------------------------------- | -------------------------------------- |\n",
    "| world   | $\\frac{1 + 1}{15 + 30} = \\frac{2}{45}$ | $\\frac{0 + 1}{15 + 30} = \\frac{1}{45}$ |\n",
    "| leaders | $\\frac{1 + 1}{15 + 30} = \\frac{2}{45}$ | $\\frac{0 + 1}{15 + 30} = \\frac{1}{45}$ |\n",
    "| agreed  | $\\frac{1 + 1}{15 + 30} = \\frac{2}{45}$ | $\\frac{1 + 1}{15 + 30} = \\frac{2}{45}$ |\n",
    "| to      | $\\frac{0 + 1}{15 + 30} = \\frac{1}{45}$ | $\\frac{0 + 1}{15 + 30} = \\frac{1}{45}$ |\n",
    "| fund    | $\\frac{0 + 1}{15 + 30} = \\frac{1}{45}$ | $\\frac{0 + 1}{15 + 30} = \\frac{1}{45}$ |\n",
    "| the     | $\\frac{2 + 1}{15 + 30} = \\frac{3}{45}$ | $\\frac{2 + 1}{15 + 30} = \\frac{3}{45}$ |\n",
    "| stadium | $\\frac{0 + 1}{15 + 30} = \\frac{1}{45}$ | $\\frac{1 + 1}{15 + 30} = \\frac{2}{45}$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I dunno about you... but I'm already exhausted trying to do this from scrach, and that's just a single sentence. Let's move into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, we want\n",
    "\n",
    "$ P(document|sports) * P(sports) $ **vs.** $ P(document|politics) * P(politics) $\n",
    "\n",
    "We already calculated $P(sports)$ and $P(politics)$ above - they're both 50% since we have an equal number of documents in each. But the $P(document | label)$ for the two labels is going to require breaking out each word to find the likelihood that each word is in each category (plus Laplacian smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_maker(category):\n",
    "    \"\"\"\n",
    "    returns the vocabulary for a given type of article\n",
    "    \"\"\"\n",
    "    vocab_category = set()\n",
    "    for art in category:\n",
    "        words = art.split()\n",
    "        for word in words:\n",
    "            vocab_category.add(word)\n",
    "    return vocab_category\n",
    "        \n",
    "voc_sports = vocab_maker(sports)\n",
    "voc_pol = vocab_maker(politics)\n",
    "total_vocabulary = voc_sports.union(voc_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocab_count = len(total_vocabulary) # useful for laplacian smoothing\n",
    "total_sports_count = len(voc_sports)\n",
    "total_politics_count = len(voc_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_number_words_in_category(phrase, category):\n",
    "    '''\n",
    "    returns number of words in the phrase previously found in the category\n",
    "    \n",
    "    inputs:\n",
    "    phrase - string, test phrase to classify\n",
    "    category - list, all training phrases associated with that category\n",
    "    \n",
    "    output:\n",
    "    word_count - default dictionary, with each word in the phrase as a key \n",
    "                 with a value of the number of times the words have \n",
    "                 appeared in the category in the train set\n",
    "    '''\n",
    "    # gets each word out - statement is a list object now\n",
    "    statement = phrase.split()\n",
    "    \n",
    "    # creating one big string from the provided category list\n",
    "    str_category=' '.join(category)\n",
    "    # splitting now so it's a single list of the words found in the category\n",
    "    cat_word_list = str_category.split()\n",
    "    # default dict allows us to create new keys easily\n",
    "    word_count = defaultdict(int) \n",
    "    \n",
    "    for word in statement:\n",
    "        for cat_word in cat_word_list:\n",
    "            if word == cat_word:\n",
    "                word_count[word] +=1\n",
    "            else:\n",
    "                word_count[word] # here's the part that works because default dict\n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sports_word_count = find_number_words_in_category(test_statement,sports)\n",
    "test_sports_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_politic_word_count = find_number_words_in_category(test_statement,politics)\n",
    "test_politic_word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ P(politics | article) = P(politics) x \\prod_{i=1}^{d} P(word_{i} | politics) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_likelihood(category_count, test_category_count, alpha):\n",
    "    \n",
    "    num = np.product(np.array(list(test_category_count.values())) + alpha)\n",
    "    denom = (category_count + total_vocab_count*alpha)**(len(test_category_count))\n",
    "    \n",
    "    return num/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "likelihood_sports = find_likelihood(total_sports_count,test_sports_word_count,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_politics = find_likelihood(total_politics_count,test_politic_word_count,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yeah... the probabilities out don't mean anything, just worry about which is bigger\n",
    "print(likelihood_sports)\n",
    "print(likelihood_politics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determing the winner of our model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/solvingforyhat.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p(politics|article) > p(music|article)\n",
    "(likelihood_politics * p_politics) > (likelihood_sports * p_sports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros:\n",
    "\n",
    "* It is an efficient way to predict class of test data set. It perform well in multi class prediction\n",
    "* When assumption of independence holds, a Naive Bayes classifier performs requires less training data and can perform better than models like logistic regression.\n",
    "* Performs better with categorical inputs. For numerical input, one has to assume a normal distribution.\n",
    "\n",
    "### Cons:\n",
    "\n",
    "* Naive Bayes is also known as a bad estimator, so the probability outputs from predict_proba are not to be taken too seriously\n",
    "* We are assuming of independent predictors, but in real life, it is almost impossible that we get a set of predictors which are completely independent (amazingly, still works a lot of the time though!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... but let's be real, we don't need to use hand-written functions for this\n",
    "\n",
    "### Using Naive Bayes in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more imports\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, plot_confusion_matrix\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching our data\n",
    "news_train = fetch_20newsgroups(subset='train', \n",
    "                                categories = ['rec.sport.baseball', \n",
    "                                              'talk.politics.misc'])\n",
    "news_test = fetch_20newsgroups(subset='test', \n",
    "                               categories = ['rec.sport.baseball', \n",
    "                                              'talk.politics.misc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting data in dataframe\n",
    "df_train = pd.DataFrame()\n",
    "df_train['Data'] = news_train.data\n",
    "df_train['Target'] = news_train.target\n",
    "\n",
    "df_test = pd.DataFrame()\n",
    "df_test['Data'] = news_test.data\n",
    "df_test['Target'] = news_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grabbing our target classes so we know which is which\n",
    "target_classes = dict(enumerate(news_test.target_names))\n",
    "target_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train Target Ratio: {df_train[\"Target\"].mean():.4f}')\n",
    "print(f'Train Target Ratio: {df_test[\"Target\"].mean():.4f}')\n",
    "# roughly equivalent breakdowns between classes in train and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Need to turn our text data into numbers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a Count Vectorizer\n",
    "# Goes through each doc and counts how many of each word\n",
    "vectorizer = CountVectorizer()\n",
    "# Fitting and transforming our train data\n",
    "X_train = vectorizer.fit_transform(df_train['Data']).toarray() # to array is just for the model later\n",
    "# Just transforming our test data\n",
    "X_test = vectorizer.transform(df_test['Data']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does this look like?\n",
    "X_train_vectorized = pd.DataFrame(X_train, columns=vectorizer.get_feature_names())\n",
    "X_train_vectorized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's explore a single example of our new vectorized X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before\n",
    "df_train.iloc[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full text before\n",
    "df_train['Data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After\n",
    "X_train_vectorized.iloc[0].sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now time to model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting our y values\n",
    "y_train = df_train['Target']\n",
    "y_test = df_test['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating our model - just using default values\n",
    "model = GaussianNB()\n",
    "# Fitting our model\n",
    "model.fit(X_train,y_train)\n",
    "# Making predictions on our test set\n",
    "y_preds = model.predict(X_test)\n",
    "\n",
    "# How'd we do?\n",
    "print(f'Naive Bayes Test Accuracy: {accuracy_score(y_test, y_preds):.4f}')\n",
    "print(f'Naive Bayes Test F1-Score: {f1_score(y_test, y_preds):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model, X_test, y_test, display_labels = target_classes.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for comparison...\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Instantiating our model - just using default values\n",
    "logreg = LogisticRegression(random_state=123)\n",
    "# Fitting our model\n",
    "logreg.fit(X_train,y_train)\n",
    "# Making predictions on our test set\n",
    "y_preds_lr = logreg.predict(X_test)\n",
    "\n",
    "# How'd we do?\n",
    "print(f'Logistic Regression Test Accuracy: {accuracy_score(y_test, y_preds_lr):.4f}')\n",
    "print(f'Logistic Regression Test F1-Score: {f1_score(y_test, y_preds_lr):.4f}')\n",
    "\n",
    "plot_confusion_matrix(logreg, X_test, y_test, display_labels = target_classes.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another comparison...\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Instantiating our model - just using default values\n",
    "rf = RandomForestClassifier(random_state=123)\n",
    "# Fitting our model\n",
    "rf.fit(X_train,y_train)\n",
    "# Making predictions on our test set\n",
    "y_preds_rf = rf.predict(X_test)\n",
    "\n",
    "# How'd we do?\n",
    "print(f'Untuned Random Forest Test Accuracy: {accuracy_score(y_test, y_preds_rf):.4f}')\n",
    "print(f'Untuned Random Forest Test F1-Score: {f1_score(y_test, y_preds_rf):.4f}')\n",
    "\n",
    "plot_confusion_matrix(rf, X_test, y_test, display_labels = target_classes.values())\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
