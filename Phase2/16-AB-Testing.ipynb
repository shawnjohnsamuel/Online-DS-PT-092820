{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Testing\n",
    "\n",
    "## AKA Applied Hypothesis Testing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you went through all the stats up to this point and thought \"oh man when am I ever going to use this stuff\" - I get it. But one of the most common ways that Hypothesis Testing techniques are used in the real world is through A/B Testing!\n",
    "\n",
    "One of the most common places you see A/B Testing out in the world is in marketing - companies will run A/B tests on elements of their website, their emails, their calls to action, etc. While you see A/B testing in other places, Marketing is going to be my example lens for today's session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A/B Testing in Marketing\n",
    "\n",
    "Hubspot is a marketing software company, and I'm going to use some of their resources in the setup to why all this matters. You can access the specific A/B Testing Kit they put out for marketing optimization process at this link: https://drive.google.com/drive/folders/1Wk3J2nA5gguN1Y_41cACxQ9mcJls9TmI\n",
    "\n",
    "Hubspot's definition of split testing, aka A/B testing:\n",
    "\n",
    "> Split testing, commonly referred to as A/B testing, is a method of testing through which marketing variables (such as copy, images, layout, etc) are compared to each other to identify the one that brings a better conversion rate. In this context, the element that is being testing is called the “control” and the element that is argued to give a better result is called the “treatment.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hubspot's 10 Guidelines for Effective A/B Testing: \n",
    "\n",
    "1. Only conduct one test (on one asset) at a time\n",
    "2. Test one variable at a time\n",
    "3. Test minor changes, too\n",
    "4. You can A/B test the entire element\n",
    "5. Measure as far down funnel as possible\n",
    "6. Set up control & treatment\n",
    "7. Decide what you want to test\n",
    "8. Split your sample group randomly \n",
    "9. Test at the same time\n",
    "10. Decide on necessary significance before testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What will the data look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data source: https://www.kaggle.com/zhangluyuan/ab-testing\n",
    "\n",
    "Unfortunately, this data has no real meta-data associated with it, but the author did say the data comes from an e-commerce website. \n",
    "\n",
    "Full credit to Robbie Geoghegan, now a Data Scientist at Facebook, for giving me the idea and sharing work they did on this dataset: https://medium.com/@robbiegeoghegan/implementing-a-b-tests-in-python-514e9eb5b3a1 \n",
    "\n",
    "Another blog I referenced: https://medium.com/@RenatoFillinich/ab-testing-with-python-e5964dd66143"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go any further, and typically before we run a test like this, we need to decide our significance level. Otherwise, let's assume that the group who ran this test did it properly (ran tests in parallel, split users randomly, etc)\n",
    "\n",
    "Significance Level: $\\alpha = .05$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from statsmodels.stats.power import NormalIndPower\n",
    "from statsmodels.stats.proportion import proportion_effectsize, proportions_ztest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab our data - want the column 'timestamp' to be a datetime object\n",
    "df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check our timeframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There's an issue..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's see..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One more thing to check..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check it out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let's explore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split out our two groups\n",
    "control_group = None\n",
    "treat_group = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of samples, timeframe and conv % for each group\n",
    "for sub_df in [control_group, treat_group]:\n",
    "    name = list(sub_df['group'])[0].title()\n",
    "    print(f\"Number of Samples in our {name} Group: {len(sub_df):,}\")\n",
    "    print(f\"Timeframe: {sub_df['timestamp'].min()} - {sub_df['timestamp'].max()}\")\n",
    "    print(f\"Number of Conversions in our {name} Group: {sub_df['converted'].sum():,}\")\n",
    "    print(f\"Conversion % in our {name} Group: {sub_df['converted'].mean() * 100:.3f}%\")\n",
    "    print(\"*\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our friend at Facebook, whose [blog](https://medium.com/@robbiegeoghegan/implementing-a-b-tests-in-python-514e9eb5b3a1) and [code](https://github.com/RobbieGeoghegan/AB_Testing/blob/master/AB_Testing.ipynb) inspired this notebook, uses two things you can determine in advance to calculate effect size:\n",
    "\n",
    "> Baseline rate — an estimate of the metric being analyzed before making any changes\n",
    "> Practical significance level — the minimum change to the baseline rate that is useful to the business, for example an increase in the conversion rate of 0.001% may not be worth the effort required to make the change whereas a 2% change will be\n",
    "\n",
    "In other words, you can determine the minimum amount of change you want to see between your two groups and use that to calculate effect size (different than calculating effect size after the study has been conducted, which isn't ideal).\n",
    "\n",
    "To do this with statsmodels, since we're doing a test on a proportion, we use: https://www.statsmodels.org/stable/generated/statsmodels.stats.proportion.proportion_effectsize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's grab some useful variables, going ahead and doing for both groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline is what we expect given what we have\n",
    "# here, we'll capture that with our percentage of conversions \n",
    "baseline_rate = None\n",
    "practical_significance = 0.01 # user defined - want at last 1% difference here\n",
    "\n",
    "effect_size = proportion_effectsize(baseline_rate, baseline_rate + practical_significance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine our minimum sample size per group\n",
    "confidence_level = 0.05 # user defined - want to be 95% confident\n",
    "power = 0.8 # user defined (1 - beta)\n",
    "\n",
    "min_sample_size = NormalIndPower().solve_power(effect_size = effect_size, \n",
    "                                               power = power, \n",
    "                                               alpha = confidence_level)\n",
    "\n",
    "print(f\"Required minimum sample size: {min_sample_size:,.0f} per group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's test!\n",
    "# Using a proportion test (not dealing with means but proportions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So?\n",
    "\n",
    "- \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
