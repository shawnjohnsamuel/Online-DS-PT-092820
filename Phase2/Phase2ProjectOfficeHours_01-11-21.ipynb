{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection & Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![baby penguin gif from Giphy](https://media.giphy.com/media/RiJuDMqd6vDgfPrZN2/giphy.gif)\n",
    "\n",
    "Let's hang out with some penguins, loading up a dataset straight from seaborn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sns.load_dataset('penguins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean up this dataset - two rows have quite a few null values, and 11 total do not have a value for `sex`, so let's drop rows where any data is null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop nulls here\n",
    "data = data.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data[['species','island','sex']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the `gender`, `species` or `island` data we need to render those strings as numbers - since there are only 2-3 unique values per column, let's simply one-hot-encode those columns (aka turn the columns into a series of binary indicators).\n",
    "\n",
    "Using Pandas' `get_dummies` : https://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.get_dummies.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode our three 'object' columns\n",
    "data_num = pd.get_dummies(data, columns=['species', 'island', 'sex'],\n",
    "                          drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll note that Pandas' `get_dummies` drops the first option, which means one of the species and one of the islands won't be obvious in our features. A thing to keep in mind!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data_num.corr().abs())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this is to predict body mass, `body_mass_g`, so let's define our X and y and perform a train/test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y\n",
    "X_cols = [c for c in data_num.columns.to_list() if c not in ['body_mass_g']]\n",
    "\n",
    "X = data_num[X_cols]\n",
    "y = data_num.body_mass_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test) + len(X_train) == len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a scaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our scaler on training data, then fit to testing\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a linear regression model\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit our model on our scaled data\n",
    "lr.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "y_train_pred = lr.predict(X_train_scaled)\n",
    "y_test_pred = lr.predict(X_test_scaled)\n",
    "\n",
    "print(\"Training Scores:\")\n",
    "print(f\"R2: {r2_score(y_train, y_train_pred)}\")\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y_train, y_train_pred)}\")\n",
    "print(\"---\")\n",
    "print(\"Testing Scores:\")\n",
    "print(f\"R2: {r2_score(y_test, y_test_pred)}\")\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y_test, y_test_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance through Coefficients\n",
    "\n",
    "Because we've scaled our data, we can explore our coefficients to see which are having more of an impact on our model.\n",
    "\n",
    "Note! This, or using p-values from a statsmodels model, is all I expect you to do in this project - anything else we talk about today is completely optional and going above and beyond!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the coefficients with the names of each col\n",
    "dict(zip(X.columns, lr.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's also look at the intercept\n",
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ELI5](https://eli5.readthedocs.io/en/latest/index.html)\n",
    "\n",
    "ELI5, short for 'explain like I'm 5', is a library specifically designed to help explore feature importances. It'll help you visualize exactly what we just did above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't have eli5? uncomment the below code\n",
    "# pip install eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(lr, feature_names=list(X.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So what?\n",
    "\n",
    "You can use feature importances, or an exploration of statistical significance through statsmodels' provided p-values, to decide which features to keep and which ones to drop in your next iteration of your model! Especially if you OHE a lot of columns, you might use this technique to grab only useful indicators rather than keeping every feature you build in your model.\n",
    "\n",
    "Will dropping seemingly 'low' features in terms of feature importance always help improve your scores? Nope! Will each way of understanding which feature is 'important' always have the same results? Nope! But try different techniques, see what you think and what helps your model, then iterate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of interpretation, you may want to get to a final model in terms of what features are used and how you process your data - then build a version of that final model on data that isn't scaled so you can see exactly how each unit of each feature directly impacts your target!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_ns = LinearRegression()\n",
    "\n",
    "lr_ns.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_train_pred = lr_ns.predict(X_train)\n",
    "y_test_pred = lr_ns.predict(X_test)\n",
    "\n",
    "# these scores should be exactly the same\n",
    "print(\"Training Scores:\")\n",
    "print(f\"R2: {r2_score(y_train, y_train_pred)}\")\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y_train, y_train_pred)}\")\n",
    "print(\"---\")\n",
    "print(\"Testing Scores:\")\n",
    "print(f\"R2: {r2_score(y_test, y_test_pred)}\")\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y_test, y_test_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# BUT the feature importances will be totally different\n",
    "eli5.show_weights(lr_ns, feature_names=list(X.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than interpret anything about this (you can NOT interpret relative feature importances based on unscaled coefficients!!), you can use these coefficients now to discuss how each unit affects the target.\n",
    "\n",
    "Example: each additional mm of flipper length adds 17.472g of body mass, being a Gentoo penguin adds 810.346g of body mass, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Above and Beyond: Regularization Terms\n",
    "\n",
    "Suppose I have split my data into training and testing sets. Do I want my model to fit my training data _exactly_?\n",
    "\n",
    "Often, the answer is actually 'NO!', because doing that will lead to an overfit model.\n",
    "\n",
    "Overfitting is generally a result of high variance. High variance can be caused by:\n",
    "\n",
    "- having irrelevant or too many predictors\n",
    "- multicollinearity\n",
    "- large coefficients\n",
    "\n",
    "The first problem is about picking up on noise rather than signal.\n",
    "\n",
    "The second problem is about having a least-squares estimate that is highly sensitive to random error.\n",
    "\n",
    "The third is about having highly sensitive predictors.\n",
    "\n",
    "Regularization works by introducing a factor into our model designed to enforce the stricture that the coefficients stay small, by penalizing the ones that get too large.\n",
    "\n",
    "That is, we'll alter our loss function so that the goal now is not merely to minimize the difference between actual values and our model's predicted values. Rather, we'll add in a term to our loss function that represents the sizes of the coefficients.\n",
    "\n",
    "There are two popular ways of doing this:\n",
    "\n",
    "Lasso (\"L1\"): Minimize $\\large\\Sigma^{n_{obs.}}_{i=1}[(y_i - \\Sigma^{n_{feat.}}_{j=0}\\beta_j\\times x_{ij})^2 + \\lambda\\Sigma^{n_{feat.}}_{j=0}|\\beta_j|]$\n",
    "<br/> <br/>\n",
    "\n",
    "Ridge (\"L2\"): Minimize $\\large\\Sigma^{n_{obs.}}_{i=1}[(y_i - \\Sigma^{n_{feat.}}_{j=0}\\beta_j\\times x_{ij})^2 + \\lambda\\Sigma^{n_{feat.}}_{j=0}\\beta^2_j]$\n",
    "\n",
    "**$\\rightarrow$ Don't let these formulas be intimidating. The first term in each of these (the sum of squares) is the same, and is just the familiar loss function that we've always used. What distinguishes the Lasso Regression from the Ridge Regression is only the extra term on the right. The Lasso uses the absolute values of the coefficients, while the Ridge uses the squares of the coefficients.**\n",
    "\n",
    "For a given value of $\\lambda$, the Ridge makes for a gentler reining in of runaway coefficients. The Lasso will more quickly reduce the contribution of individual predictors down to insignificance.\n",
    "\n",
    "For a nice discussion of these methods in Python, see https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b.\n",
    "\n",
    "**TL;DR:**\n",
    "\n",
    "- L1 Regularization (LASSO) is good for feature selection\n",
    "\n",
    "- L2 Regularization (Ridge) is good for reducing the impact of multicollinear features\n",
    "\n",
    "(also, you can do both at the same time with a technique called [Elastic Net](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 Norm: [LASSO](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)\n",
    "\n",
    "(Least Absolute Shrinkage and Selection Operator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a lasso regression model\n",
    "lasso = Lasso(alpha=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit your new L1 model -  on the scaled data\n",
    "lasso.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "y_train_pred_l1 = lasso.predict(X_train_scaled)\n",
    "y_test_pred_l1 = lasso.predict(X_test_scaled)\n",
    "\n",
    "print(\"Training Scores:\")\n",
    "print(f\"R2: {r2_score(y_train, y_train_pred_l1)}\")\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y_train, y_train_pred_l1)}\")\n",
    "print(\"---\")\n",
    "print(\"Testing Scores:\")\n",
    "print(f\"R2: {r2_score(y_test, y_test_pred_l1)}\")\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y_test, y_test_pred_l1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember - what's the benefit of using LASSO?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Unpenalized Linear Regression Coefficients are:{}\".format(lr.coef_))\n",
    "print(\"Unpenalized Linear Regression Intercept:{}\".format(lr.intercept_))\n",
    "print(\"---\")\n",
    "print(\"Lasso Regression Coefficients are:{}\".format(lasso.coef_))\n",
    "print(\"Lasso Linear Regression Intercept:{}\".format(lasso.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Norm: [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a lasso regression model\n",
    "ridge = Ridge(alpha=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit your new L2 model -  on the scaled data\n",
    "ridge.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "y_train_pred_l2 = ridge.predict(X_train_scaled)\n",
    "y_test_pred_l2 = ridge.predict(X_test_scaled)\n",
    "\n",
    "print(\"Training Scores:\")\n",
    "print(f\"R2: {r2_score(y_train, y_train_pred_l2)}\")\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y_train, y_train_pred_l2)}\")\n",
    "print(\"---\")\n",
    "print(\"Testing Scores:\")\n",
    "print(f\"R2: {r2_score(y_test, y_test_pred_l2)}\")\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y_test, y_test_pred_l2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unpenalized Linear Regression Coefficients are:{}\".format(lr.coef_))\n",
    "print(\"Unpenalized Linear Regression Intercept:{}\".format(lr.intercept_))\n",
    "print(\"---\")\n",
    "print(\"Ridge Regression Coefficients are:{}\".format(ridge.coef_))\n",
    "print(\"Ridge Linear Regression Intercept:{}\".format(ridge.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_dict = {}\n",
    "for loc, col in enumerate(data_num.columns):\n",
    "    coef_dict[col] = {\"Unpenalized\": lr.coef_[loc-1],\n",
    "                      \"LASSO\": lasso.coef_[loc-1],\n",
    "                      \"Ridge\": ridge.coef_[loc-1]}\n",
    "pd.DataFrame.from_dict(coef_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alpha Levels??\n",
    "\n",
    "We started with the **hyperparameter** alpha set to `0.5` for both our LASSO and Ridge Models: now let's play around with it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [.25, .5, 1, 10, 50, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in alphas:\n",
    "    lasso_testing = Lasso(alpha = a)\n",
    "    lasso_testing.fit(X_train_scaled, y_train)\n",
    "    y_train_pred_testing = lasso_testing.predict(X_train_scaled)\n",
    "    y_test_pred_testing = lasso_testing.predict(X_test_scaled)\n",
    "\n",
    "    print(f\"Training Scores at alpha: {a}\")\n",
    "    print(f\"R2: {r2_score(y_train, y_train_pred_testing)}\")\n",
    "    print(f\"Mean Absolute Error: {mean_absolute_error(y_train, y_train_pred_testing)}\")\n",
    "    print(\"---\")\n",
    "    print(f\"Testing Scores at alpha: {a}\")\n",
    "    print(f\"R2: {r2_score(y_test, y_test_pred_testing)}\")\n",
    "    print(f\"Mean Absolute Error: {mean_absolute_error(y_test, y_test_pred_testing)}\")\n",
    "    print(\"---\")\n",
    "    print(f\"Coefficients at alpha: {a} are: {lasso_testing.coef_}\")\n",
    "    print(f\"Intercept at alpha: {a} is:{lasso_testing.intercept_}\")\n",
    "    print(\"*\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in alphas:\n",
    "    ridge_testing = Ridge(alpha = a)\n",
    "    ridge_testing.fit(X_train_scaled, y_train)\n",
    "    y_train_pred_testing = ridge_testing.predict(X_train_scaled)\n",
    "    y_test_pred_testing = ridge_testing.predict(X_test_scaled)\n",
    "\n",
    "    print(f\"Training Scores at alpha: {a}\")\n",
    "    print(f\"R2: {r2_score(y_train, y_train_pred_testing)}\")\n",
    "    print(f\"Mean Absolute Error: {mean_absolute_error(y_train, y_train_pred_testing)}\")\n",
    "    print(\"---\")\n",
    "    print(f\"Testing Scores at alpha: {a}\")\n",
    "    print(f\"R2: {r2_score(y_test, y_test_pred_testing)}\")\n",
    "    print(f\"Mean Absolute Error: {mean_absolute_error(y_test, y_test_pred_testing)}\")\n",
    "    print(\"---\")\n",
    "    print(f\"Coefficients at alpha: {a} are: {ridge_testing.coef_}\")\n",
    "    print(f\"Intercept at alpha: {a} is:{ridge_testing.intercept_}\")\n",
    "    print(\"*\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "- [Stats course resource from Penn State](https://online.stat.psu.edu/stat508/lesson/5), going into detail about Regression Shrinkage Methods - aka regularization. This is pretty technical, and the code is in R, but goes into good detail about the motivation of why we do this and how this works.\n",
    "\n",
    "- Kaggle has a course on Machine Learning interpretability, with a section on [Permutation Importance](https://www.kaggle.com/dansbecker/permutation-importance) if you'd like to explore that"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
