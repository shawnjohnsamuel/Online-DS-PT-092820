{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null and Alternative Hypotheses\n",
    "\n",
    "A pretty painted picture of science posits that a scientist formulates a hypothesis that explains or generalizes from some set of observations, and then conducts some experiment, which will either confirm or refute that hypothesis.\n",
    "\n",
    "A nice simplification, but an oversimplification. Consider the possibility that the experiment yields statistically improbable results. In that case it may well be a mistake to generalize from those results or to reject a hypothesis that doesn't predict them.\n",
    "\n",
    "Often the confirmation of some testing/experiment/ **alternative hypothesis, $H_\\alpha$**, is a _relative_ affair, where it is measured against some **null hypothesis, $H_0$**.\n",
    "\n",
    "If an alternative hypothesis states that there is some significant relationship between two variables, then the null hypothesis simply states that there is no such relationship.\n",
    "\n",
    "If we're testing the function of a new drug, then the null hypothesis will say that the drug has _no effect_ on patients, or anyway no effect relative to relief of the malady the drug was designed to combat. If we're testing whether Peeps cause dementia, then the null hypothesis will say that there is _no correlation_ between Peeps consumption and rate of dementia development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $p$-Values\n",
    "\n",
    "The basic idea of a p-value is to quantify the probability that the results seen are in fact the result of mere random chance. This is connected with the null hypothesis since, if the null hypothesis is true and there is no significant correlation between the population variables X and Y, then of course any correlation between X and Y observed in our sample would have to be the result of mere random chance.\n",
    "\n",
    "### How Unlikely Is *Too* Unlikely?\n",
    "\n",
    "Suppose we calculate a p-value for some statistic we've measured (more on this below!) and we get a p-value of 20%. This would mean that there is a 20% chance that the results we observed were the result of mere random chance. Probably this is high enough that we ought _not_ to reject the null hypothesis that our variables are uncorrelated.\n",
    "\n",
    "In practice, a p-value _threshold_ of 5% is very often the default value for these tests of statistical significance. Thus, if it is calculated that the chance that the results we observed were actually the result of randomness is less than 1 in 20, then we would _reject_ the null hypothesis and _accept_ the alternative hypothesis.\n",
    "\n",
    "### Definitions\n",
    "\n",
    "#### Significance Level $\\alpha$\n",
    "\n",
    "The significance level $\\alpha$ is the threshold at which you're okay with rejecting the null hypothesis. It is the probability of rejecting the null hypothesis when it is true.\n",
    "\n",
    "The most commonly used significance level is $\\alpha = 0.05$. When you set $\\alpha = 0.05$, you're saying \"I'm okay with rejecting the null hypothesis if there is less than a 5% chance that the results I am seeing are actually due to randomness\".\n",
    "\n",
    "#### Test Statistic\n",
    "\n",
    "\"The test statistic takes your data from an experiment or survey and compares your results to the results you would expect from the null hypothesis.\"\n",
    "\n",
    "-- [Statistic How-To](https://www.statisticshowto.com/test-statistic/)\n",
    "\n",
    "This test statistic is what exactly you calculate, and then you use the threshold significance level to decide whether this test statistic is large enough. \n",
    "\n",
    "\"The larger the t score, the larger the difference is between the groups you are testing\" (also from [Statistics How-To](https://www.statisticshowto.com/probability-and-statistics/t-distribution/t-score-formula/))\n",
    "\n",
    "#### p-values\n",
    "\n",
    "The p-value is the probability of observing a test statistic at least as large as the one observed, by random chance, assuming that the null hypothesis is true.\n",
    "\n",
    "If $p \\lt \\alpha$, we reject the null hypothesis.\n",
    "\n",
    "If $p \\geq \\alpha$, we fail to reject the null hypothesis.\n",
    "\n",
    "> *We do **not** accept the alternative hypothesis, we only **reject** or **fail to reject** the null hypothesis in favor of the alternative.*\n",
    "\n",
    "\n",
    "\n",
    "### What P-Values Are, and What They Aren't\n",
    "\n",
    "There's a trend in stats right now of criticizing P-values, so you may see some criticism of using P-values to conduct tests. Yudi Pawitan, who works in Medical Epidemiology and Biostatistics at the Karolinska Institutet in Stockholm, Sweden, was recently on Data Skeptic to discuss his recent paper: _Defending the P-value_.\n",
    "\n",
    "If you want to learn more about the controversy, and what P-values are and what they aren't, I recommend you give the episode a listen:\n",
    "\n",
    "https://podcasts.apple.com/us/podcast/defending-the-p-value/id890348705?i=1000494460371\n",
    "\n",
    "The point: scientists often don't do enough work thinking through what p-value _threshold_ they should use, which can lead to problems. Often the standard is 5% (.05) - but while that works fine for some areas of research, that might be too low or too high for others. \n",
    "\n",
    "P-values more than anything are way of balancing between false positives and false negatives, which we'll discuss more later in this notebook. But, when deciding your threshold, you should think through the cost of your false positive versus the cost of your false negative, rather than using some arbitrary standard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So, The Tests\n",
    "\n",
    "Statistical hypothesis tests, at their core, check for statistically significant differences. But there are plenty of varieties...\n",
    "\n",
    "### One-sample z-test\n",
    "\n",
    "For large enough sample sizes (at least $n$ =30), with known population standard deviation $\\sigma$, the _test statistic_ of the sample mean $\\bar x$ is given by the z-statistic,\n",
    "\n",
    "$$Z = \\frac{\\bar{x} - \\mu}{\\sigma/\\sqrt{n}}$$\n",
    "\n",
    "where $\\mu$ is the population mean.\n",
    "\n",
    "Our hypothesis test tries to answer the question of how likely we are to observe a z-statistic as extreme as our sample's given the null hypothesis that the sample and the population have the same mean, given a significance threshold of $\\alpha$. This is a one-sample z-test.\n",
    "\n",
    "### One-sample t-test\n",
    "\n",
    "For small sample sizes or samples with unknown population standard deviation, the _test statistic_ of the sample mean is given by the t-statistic,\n",
    "\n",
    "$$ t = \\frac{\\bar{x} - \\mu}{s/\\sqrt{n}} $$\n",
    "\n",
    "Here, $s$ is the sample standard deviation, which is used to estimate the population standard deviation, $\\bar{x}$ is the sample mean, and $\\mu$ is the population mean.\n",
    "\n",
    "Our hypothesis test tries to answer the question of how likely we are to observe a t-statistic as extreme as our sample's given the null hypothesis that the sample and population have the same mean, given a significance threshold of $\\alpha$. This is a one-sample t-test.\n",
    "\n",
    "### Two-sample t-tests\n",
    "\n",
    "Sometimes, we are interested in determining whether two population means are equal. In this case, we use two-sample t-tests.\n",
    "\n",
    "There are **two types** of two-sample t-tests: paired and independent (unpaired) tests.\n",
    "\n",
    "#### What's the difference?\n",
    "\n",
    "**Paired tests:** How is a sample affected by a certain treatment? The individuals in the sample remain the same and you compare how they change after treatment.\n",
    "\n",
    "**Independent tests:** When we compare two different, unrelated samples to each other, we use an independent (or unpaired) two-sample t-test.\n",
    "\n",
    "The _test statistic_ for an unpaired two-sample t-test is slightly different than the test statistic for the one-sample t-test.\n",
    "\n",
    "Assuming equal variances, the test statistic for a two-sample t-test is given by:\n",
    "\n",
    "$$ t = \\frac{\\bar{x_1} - \\bar{x_2}}{\\sqrt{s^2 \\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)}}$$\n",
    "where $s^2$ is the pooled sample variance,\n",
    "\n",
    "$$ s^2 = \\frac{\\sum_{i=1}^{n_1} \\left(x_i - \\bar{x_1}\\right)^2 + \\sum_{j=1}^{n_2} \\left(x_j - \\bar{x_2}\\right)^2 }{n_1 + n_2 - 2} $$\n",
    "\n",
    "Here, $n_1$ is the sample size of sample 1 and $n_2$ is the sample size of sample 2.\n",
    "\n",
    "An independent two-sample t-test for samples of size $n_1$ and $n_2$ has $(n_1 + n_2 - 2)$ degrees of freedom.\n",
    "\n",
    "(I know, this looks like a ton of terrible math, but we're doing all this in Python, fear not)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision rule\n",
    "\n",
    "<img src=\"https://github.com/learn-co-students/dsc-hypothesis_testing-seattle-102819/raw/633b48d10c99c4d75ba7d2ceadcf67b5f49c9c8d/images/hypothesis_test.png\" alt=\"image comparing 1-tailed and 2-tailed tests\" width=500>\n",
    "\n",
    "**Upper-tailed test (right-tailed test):**\n",
    "\n",
    "   - The null hypothesis is rejected if the **test statistic is greater than the critical value**.\n",
    "\n",
    "**Lower-tailed test (left-tailed test):**\n",
    "\n",
    "   - The null hypothesis is rejected if the **test statistic is smaller than the critical value**.\n",
    "\n",
    "**Two-tailed test:**\n",
    "\n",
    "   - The null hypothesis is rejected if the **test statistic is either larger than an upper critical value or smaller than a lower critical value**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Example\n",
    "\n",
    "Let's work with the normal distribution, since it's so useful. Suppose we are told that African elephants have weights distributed normally around a mean of 9000 lbs., with a standard deviation of 900 lbs. Pachyderm Adventures has recently measured the weights of 45 African elephants in Gabon and has calculated their average weight at 8637 lbs. They claim that these statistics on the Gabonese elephants are significant. Let's find out!\n",
    "\n",
    "What is our null hypothesis here?\n",
    "\n",
    "> $H_0$: \n",
    "\n",
    "What is our alternative hypothesis here?\n",
    "\n",
    "> $H_a$: \n",
    "\n",
    "Based on our alternative hypothesis, what kind of test is this?\n",
    "\n",
    "> ?\n",
    "\n",
    "What threshold should we set?\n",
    "\n",
    "> $\\alpha$ = \n",
    "\n",
    "Anything else?\n",
    "\n",
    "> ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool cool... now let's get to the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T22:50:24.276819Z",
     "start_time": "2020-07-30T22:50:24.235649Z"
    }
   },
   "outputs": [],
   "source": [
    "#critical z-statistic \n",
    "alpha = None\n",
    "\n",
    "# point percent function is the inverse of the cumulative density\n",
    "# function which can be understood as the quantile\n",
    "\n",
    "stats.norm.ppf(alpha/2), stats.norm.ppf(1-alpha/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is showing the cutoff points, the thresholds, for a normal distribution with an alpha of .05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to calculate our test statistic:\n",
    "\n",
    "$$\\text{z-statistic} = \\frac{\\bar{x} - \\mu}{\\sigma/\\sqrt{n}}, $$\n",
    "\n",
    "where $\\bar x$ is the sample mean, $\\mu$ is the population mean, $\\sigma$ is the population standard deviation, and $n$ is the sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T22:54:20.110311Z",
     "start_time": "2020-07-30T22:54:20.107028Z"
    }
   },
   "outputs": [],
   "source": [
    "n = None \n",
    "sigma = None \n",
    "\n",
    "x_bar = None\n",
    "mu = None\n",
    "\n",
    "se = sigma/np.sqrt(n) # our denominator is standard error\n",
    "z = (x_bar - mu)/se \n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reject or fail to reject the null hypothesis?** \n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type 1 Errors (False Positives) and Type 2 Errors (False Negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most tests for the presence of some factor are imperfect. And in fact most tests are imperfect in two ways: They will sometimes fail to predict the presence of that factor when it is after all present, and they will sometimes predict the presence of that factor when in fact it is not. Clearly, the lower these error rates are, the better, but it is not uncommon for these rates to be between 1% and 5%, and sometimes they are even higher than that. (Of course, if they're higher than 50%, then we're better off just flipping a coin to run our test!)\n",
    "\n",
    "Predicting the presence of some factor (i.e. counter to the null hypothesis) when in fact it is not there (i.e. the null hypothesis is true) is called a **\"false positive\"**. Failing to predict the presence of some factor (i.e. in accord with the null hypothesis) when in fact it is there (i.e. the null hypothesis is false) is called a **\"false negative\"**.\n",
    "\n",
    "![basic confusion matrix](images/false-pos-false-neg.jpg)\n",
    "\n",
    "[Image Source](https://www.bugseng.com/content/what-are-costs-false-positives-and-false-negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Rough-and-Tumble Recap to Statistical Hypothesis Testing:\n",
    "\n",
    "1. Start with a Scientific Question (yes/no)\n",
    "2. Take the skeptical stance (null hypothesis)\n",
    "3. State the complement (alternative hypothesis)\n",
    "4. Create a model of the situation **assuming the null hypothesis is true!**\n",
    "5. Decide how surprised you would need to be in order to change your mind (alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And now, with data\n",
    "\n",
    "Alright, millenials - your love for avocados is legendary. So let's grab some data on avocado prices by region and use them to practice statistics!\n",
    "\n",
    "<img src=\"images/Avocado-cantpleaseeveryone.jpg\" alt=\"Avocado meme: You can't please everyone, you're not an avocado\" width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More imports\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another example, now with data\n",
    "\n",
    "Okay, let's say we have two sets of regions: the Northern hemisphere and the Southern hemisphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sample data from each hemisphere\n",
    "northern_hem = pd.read_csv('data/northern_hemisphere.csv')\n",
    "southern_hem = pd.read_csv('data/southern_hemisphere.csv')\n",
    "\n",
    "# Plot histograms\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.set_title('Sample of Average Avocado Prices: Northern Hemisphere')\n",
    "ax1.set_xlabel('Average Price')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.hist(northern_hem['Average Price'], bins=20)\n",
    "\n",
    "ax2.set_title('Sample of Average Avocado Prices: Southern Hemisphere')\n",
    "ax2.set_xlabel('Average Price')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.hist(southern_hem['Average Price'], bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to know if people in the Southern hemisphere spend **a different amount** on avocados on average than those in the Northern hemisphere.\n",
    "\n",
    "First, let's answer a few questions:\n",
    "\n",
    "1. What **kind of test** are we doing? \n",
    "\n",
    "    - \n",
    "\n",
    "2. What are our **Null** and **Alternative** hypotheses? \n",
    "\n",
    "    - \n",
    "    \n",
    "3. What would a **Type I** error look like in this context?\n",
    "\n",
    "    - \n",
    "\n",
    "4. What would a **Type II** error look like in this context?\n",
    "\n",
    "    - \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a statistical test on the two samples. Can you reject the null hypothesis?\n",
    "\n",
    "Use a significance level of $\\alpha = 0.05$. You can assume the two samples have equal variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reject or fail to reject the null hypothesis?** \n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we only want to know if people in the Southern hemisphere spend **less** than those in the Northern hemisphere? What does that change?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [The ultimate cheat sheet for statistical tests in Python, from Machine Learning Mastery](https://machinelearningmastery.com/statistical-hypothesis-tests-in-python-cheat-sheet/)\n",
    "- [Here's that Data Skeptic podcast link again, on 'Defending the p-value'](https://podcasts.apple.com/us/podcast/defending-the-p-value/id890348705?i=1000494460371)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
