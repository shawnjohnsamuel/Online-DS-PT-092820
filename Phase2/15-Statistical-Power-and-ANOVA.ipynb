{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Power, and Analysis of Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T20:37:20.194588Z",
     "start_time": "2020-08-07T20:37:19.088892Z"
    }
   },
   "outputs": [],
   "source": [
    "# Overall Imports\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "np.random.seed(10)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Power\n",
    "\n",
    "At its core, the statistical power of a test is simply $1 - \\beta$, where $\\beta$ is the chance of making a Type II Error.\n",
    "\n",
    "<img src=\"images/confusionmatrix.png\" alt=\"type 1 and type 2 errors in a confusion matrix chart\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical power captures the likelihood that you'll find what you're looking for - so it makes sense that you'll only run a test if there's a likelihood you'll find results. \n",
    "\n",
    "Statistical power can be calculated if you know three things:\n",
    "\n",
    "- Significance level ($\\alpha$, which you set)\n",
    "- Effect size (the difference in distributions, which we'll go into in a second)\n",
    "- Sample size (number of observations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Effect Size\n",
    "Effect size' is used to quantify the size of the difference between two groups under observation. Effect sizes are easy to calculate, understand and apply to any measured outcome and is applicable to a multitude of study domains. It is highly valuable towards quantifying the effectiveness of a particular intervention, relative to some comparison. Measuring effect size allows scientists to go beyond the obvious and simplistic, 'Does it work or not?' to the far more sophisticated, 'How well does it work in a range of contexts?'."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Where does gender have a larger effect?\n",
    "\n",
    "![gender effect size in seals vs pugs](images/gendereffectsize.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Specifically, knowing the effect size helps you with:\n",
    "\n",
    "- Communicate practical significance of results. An effect might be statistically significant, but does it matter in practical scenarios ?\n",
    "\n",
    "- Effect size calculation and interpretation allows you to draw Meta-Analytical conclusions. This allows you to group together a number of existing studies, calculate the meta-analytic effect size and get the best estimate of the tur effect size of the population.\n",
    "\n",
    "- Perform Power Analysis , which help determine the number of particicpants (sample size) that a study would require to achieve a certain probability of finding a true effect - if there is one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example\n",
    "Compare effect size of gender in height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# calculate and visualize effect size in python \n",
    "# Import necessary modules \n",
    "# from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Mean height and sd for males\n",
    "male_mean = 178\n",
    "male_sd = 7.7\n",
    "\n",
    "# Generate a normal distribution for male heights \n",
    "male_height = stats.norm(male_mean, male_sd)\n",
    "\n",
    "# now for females\n",
    "female_mean = 163\n",
    "female_sd = 7.3\n",
    "female_height = stats.norm(female_mean, female_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_PDF(rv, x=4):\n",
    "    '''\n",
    "    Input: a random variable object, standard deviation\n",
    "    output : x and y values for the normal distribution\n",
    "    '''\n",
    "    \n",
    "    # Identify the mean and standard deviation of random variable \n",
    "    mean = rv.mean()\n",
    "    std = rv.std()\n",
    "\n",
    "    # Use numpy to calculate evenly spaced numbers over the specified interval (4 sd) and generate 100 samples.\n",
    "    xs = np.linspace(mean - x*std, mean + x*std, 100)\n",
    "    \n",
    "    # Calculate the peak of normal distribution i.e. probability density. \n",
    "    ys = rv.pdf(xs)\n",
    "\n",
    "    return xs, ys # Return calculated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Male height\n",
    "mxs, mys = evaluate_PDF(male_height)\n",
    "plt.plot(mxs, mys, label='male', linewidth=4, color='#beaed4') \n",
    "\n",
    "#Female height \n",
    "fxs, fys = evaluate_PDF(female_height)\n",
    "plt.plot(fxs, fys, label='female', linewidth=4, color='#fdc086')\n",
    "\n",
    "plt.xlabel('height (cm)')\n",
    "plt.ylabel('probability density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cohen's $d$, standardized metric for effect size\n",
    "\n",
    "Cohenâ€™s $d$ is one of the most common ways to measure effect size. As an effect size, Cohen's d is typically used to represent the magnitude of differences between two (or more) groups on a given variable, with larger values representing a greater differentiation between the two groups on that variable.\n",
    "\n",
    "$d$ = effect size (difference of means) / pooled standard deviation;\n",
    "\n",
    "$d = \\frac{\\mu1 - \\mu2}{\\sigma pooled}$\n",
    "\n",
    "The power is __not__ the same as $d$, but it is dependent on the expected t value,represented by $\\delta$, calculated by:\n",
    "\n",
    "$\\delta = $d$\\sqrt \\frac{n}{2}$\n",
    "\n",
    "We can then use the power table to find the power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def Cohen_d(group1, group2):\n",
    "    '''\n",
    "    Compute Cohen's d\n",
    "\n",
    "    group1: Series or NumPy array\n",
    "    group2: Series or NumPy array\n",
    "\n",
    "    returns: float, for Cohen's d \n",
    "    '''\n",
    "\n",
    "    diff = group1.mean() - group2.mean()\n",
    "\n",
    "    n1 = len(group1)\n",
    "    n2 = len(group2)\n",
    "    var1 = group1.var()\n",
    "    var2 = group2.var()\n",
    "\n",
    "    # Calculate the pooled variance\n",
    "    pooled_var = (n1 * var1 + n2 * var2) / (n1 + n2 - 2)\n",
    "    \n",
    "    # Calculate Cohen's d statistic\n",
    "    d = diff / np.sqrt(pooled_var)\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# x1 = np.random.normal(male_mean, male_sd, 1000)\n",
    "# x2 = np.random.normal(female_mean, female_sd, 1000)\n",
    "\n",
    "# grab 1000 random variables from each distribution\n",
    "female_sample = female_height.rvs(1000)\n",
    "male_sample = male_height.rvs(1000)\n",
    "\n",
    "effect = Cohen_d(male_sample, female_sample)\n",
    "print(effect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Evaluating Effect Size\n",
    "\n",
    "[good demo here](https://rpsychologist.com/d3/cohend/)\n",
    "\n",
    "In general:\n",
    "\n",
    "- Small effect = 0.2\n",
    "- Medium Effect = 0.5\n",
    "- Large Effect = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_pdfs(cohen_d=2):\n",
    "    \"\"\"\n",
    "    Plot PDFs for distributions that differ by some number of stds.\n",
    "    \n",
    "    cohen_d: number of standard deviations between the means\n",
    "    \"\"\"\n",
    "    group1 = stats.norm(0, 1) # assume mean = 0 and std = 1\n",
    "    group2 = stats.norm(cohen_d, 1)\n",
    "    xs, ys = evaluate_PDF(group1)\n",
    "    plt.fill_between(xs, ys, label='Female', color='#ff2289', alpha=0.7)\n",
    "\n",
    "    xs, ys = evaluate_PDF(group2)\n",
    "    plt.fill_between(xs, ys, label='Male', color='#376cb0', alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_pdfs(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pdfs(.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pdfs(.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pdfs(effect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Error\n",
    "\n",
    "When conducting hypothesis testing, we __choose__ a value for alpha, which represents the margin of error we are allowing. Remember, alpha represents the probability in which we are allowed to take the risk of falsely rejecting the null hypothesis. By convention, we set the alpha at 0.05, which we can interpret as \"for 5% of the time, we are willing to reject the null hypothesis when it is in fact true\". How, then, do we categorize different types of error associated with conducting the experiments?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Type I error\n",
    "Type I error is usually represented as $\\alpha$, which is the probability of rejecting the null hypothesis when it is in fact true. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For example, if we compare the height of males and females in a sample, with $\\mu$m = 175cm, $\\sigma$m = 4cm, and $\\mu$f = 170 cm, $\\sigma$f = 2.5cm, the __null hypothesis__ would be: there is no difference of heights in two genders. After conducting the appropriate statistical testing and attaining an alpha of .04, we conclude that there is a significant difference in gender. However, the reality is there is no difference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Type II Error\n",
    "Type II error is represented as $\\beta$, it is the probability of failing to reject the null when it is in fact false. Applying to our case above, we would have said that the two genders do not differ in heights when they in fact do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tie it all back together}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Elements that affect power:\n",
    "- Effect Size\n",
    "- Sample Size (and thus Standard Error)\n",
    "- Alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How does sample size affect power?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Well, let's find out!\n",
    "from statsmodels.stats.power import TTestIndPower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.add_subplot(2,1,1)\n",
    "fig = TTestIndPower().plot_power(dep_var='nobs',\n",
    "                                 nobs= np.arange(2, 200),\n",
    "                                 effect_size=np.array([0.2, 0.5, 0.8]),\n",
    "                                 alpha=0.01,\n",
    "                                 ax=ax, title='Power of t-Test' + '\\n' + r'$\\alpha = 0.01$')\n",
    "ax.get_legend().remove()\n",
    "ax = fig.add_subplot(2,1,2)\n",
    "fig = TTestIndPower().plot_power(dep_var='nobs',\n",
    "                                 nobs= np.arange(2, 200),\n",
    "                                 effect_size=np.array([0.2, 0.5, 0.8]),\n",
    "                                 alpha=0.05,\n",
    "                                 ax=ax, title=r'$\\alpha = 0.05$') \n",
    "plt.tight_layout()\n",
    "plt.legend(loc=(1.05,2.2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quick Case Study \n",
    "\n",
    "Suppose you are launching a pilot study with Instagram and you want to examine the new feature (making the heart when you \"like\" someone's photo red instead of white) developed by the frontend engineer attracted more likes __given__ other variables are being held constant. You have collected two datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "experiment = pd.read_csv('data/ig_experiment.csv', index_col=0)\n",
    "control = pd.read_csv('data/ig_control.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "experiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "control.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Solve for sample size\n",
    "\n",
    "We can use `power_analysis.solve_power` from `statsmodels` to find the sample size you need.\n",
    "\n",
    "[documentation here](https://www.statsmodels.org/dev/generated/statsmodels.stats.power.tt_ind_solve_power.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_size = Cohen_d(control['Likes_Given_Con'], experiment['Likes_Given_Exp'])\n",
    "effect_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# before we even start the experiment, we want to know in order to attain a power of .8 \n",
    "# given an alpha of .05, how many observations we need \n",
    "alpha = 0.05 # significance level\n",
    "power = 0.8\n",
    "\n",
    "power_analysis = TTestIndPower()\n",
    "sample_size = power_analysis.solve_power(effect_size = effect_size, \n",
    "                                         power = power, \n",
    "                                         alpha = alpha)\n",
    "sample_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANOVA \n",
    "\n",
    "ANOVA, short for **An**alysis **o**f **Va**riance, is a commonly used statistical method for comparing means using the calculated F-statistic of 3 groups or more.  \n",
    "\n",
    "<center><img src='images/rsz_anova-800x444.jpg'>\n",
    "    \n",
    "Like all tests, we calculate a statistic (F-ratio or F-statistic) to get a p-value to compare with the critical value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at an example\n",
    "\n",
    "A company is wondering how they can best optimize the performance of their data scientists. They devise an experiment to test the effect of various substances on the quality of work completed by their data scientists. They come up with four groups:\n",
    "\n",
    "      Group A:  Given 150mg of caffeine\n",
    "      Group B:  Given 2 ounces of alcohol\n",
    "      Group C:  Given 100g of chocolate\n",
    "      Group D:  Given 10 ounces of water\n",
    "After ingesting their given substance, each data scientist was then given the same assessment. After two hours, the accuracy of each data scientist's model was evaluated and recorded and the results are shown below. With a confidence level of 95%, is there a difference in performance across the groups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T20:37:42.397555Z",
     "start_time": "2020-08-07T20:37:42.394075Z"
    }
   },
   "outputs": [],
   "source": [
    "A = [0.92, 0.89, 0.94, 0.91, 0.79, 0.90, 0.96, 0.94, 0.92, 0.85]\n",
    "B = [0.65, 0.79, 0.99, 0.48, 0.54, 0.68, 0.52, 0.49, 0.52, 0.56]\n",
    "C = [0.85, 0.89, 0.91, 0.92, 0.86, 0.82, 0.94, 0.90, 0.91, 0.95]\n",
    "D = [0.69, 0.75, 0.85, 0.74, 0.76, 0.85, 0.78, 0.72, 0.84, 0.86]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T20:38:16.687078Z",
     "start_time": "2020-08-07T20:38:16.683139Z"
    }
   },
   "outputs": [],
   "source": [
    "f_stat,p_value = stats.f_oneway(A,B,C,D)\n",
    "print('F-stat:',f_stat)\n",
    "print('p-value:',p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the f-statistic? \n",
    "<center><img src='images/f-stat.png'>\n",
    "    \n",
    "The test statistic for ANOVA follows the F-distribution, a continuous probability function with 2 unique values, the degrees of freedom of groups and the degrees of freedom of all subjects. It is positively skewed and defined only for positive values. \n",
    "\n",
    "If, a = number of groups AND N = total number of subjects THEN \n",
    "- Degrees of freedom numerator = a - 1\n",
    "- Degrees of freedom denominator = N - a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T20:49:51.935817Z",
     "start_time": "2020-08-07T20:49:51.793837Z"
    }
   },
   "outputs": [],
   "source": [
    "# here's one example\n",
    "x = np.linspace(0,5,1000)\n",
    "y = stats.f.pdf(x,3,36)\n",
    "plt.plot(x,y)\n",
    "plt.title('F-distribution dfn=3,dfd=36')\n",
    "plt.xlabel('F-statistic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T20:51:32.017829Z",
     "start_time": "2020-08-07T20:51:31.845361Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example of many f-distributions\n",
    "x = np.linspace(0,5,1000)\n",
    "y = stats.f.pdf(x,3,16)\n",
    "plt.plot(x,y)\n",
    "y = stats.f.pdf(x,3,3)\n",
    "plt.plot(x,y)\n",
    "y = stats.f.pdf(x,16,3)\n",
    "plt.plot(x,y)\n",
    "y = stats.f.pdf(x,16,16)\n",
    "plt.plot(x,y)\n",
    "plt.legend(['dfn=3,dfd=17','dfn=3,dfd=3','dfn=17,dfd=3','dfn=17,dfd=17'])\n",
    "plt.title('F-distribution')\n",
    "plt.xlabel('F-statistic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-Test or ANOVA?\n",
    "\n",
    "Suppose we want to compare whether multiple groups differ in some type of measures. For example, we have collected mood data grouped by four types of weather - sunny, raining, overcast, or snowy, and we want to find out whether there is a difference in mood across different weather. What tests would you use?\n",
    "\n",
    "A natural reaction would be to conduct multiple t-tests. However, that comes with many drawbacks. First, you would need $\\frac{n(n-1)}{2}$ t tests, which come out to 6 tests. Having more tests meaning you create a higher chance of making type I errors. In this case, our original probability of making type I error grew from 5% to 5% x 6 = 30%! By conduct 6 tests and comparing their mean to each other, we are running a huge risk of believing in false positives. \n",
    "\n",
    "How then, can we combat this? **ANOVA**!\n",
    "\n",
    "Instead of looking at each individual difference, ANOVA examines the ratio of variance between groups, and variance within groups, and find out whether the ratio is big enough to be statistically significant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T-Test Statistics\n",
    "\n",
    "## $$t = \\frac{x\\bar - \\mu}{\\frac{s}{\\sqrt n}}$$\n",
    "\n",
    "#### ANOVA - the F test\n",
    "\n",
    "## $$F = \\frac{MS_{between}}{MS_{within}}$$\n",
    "\n",
    "We can also say that a t-test is a special case of ANOVA, in that we are comparing the means of only two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/bikeshare_day.csv')\n",
    "data.head()\n",
    "# cnt is the outcome we are trying to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping the season names onto the data\n",
    "seasons = {1: 'spring',\n",
    "           2: 'summer',\n",
    "           3: 'fall',\n",
    "           4: 'winter'}\n",
    "data['season_cat'] = data.season.map(seasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# boxplot\n",
    "sns.boxplot(x='season_cat', y='cnt', data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spring = data.loc[data['season'] == 1]['cnt']\n",
    "summer = data.loc[data['season'] == 2]['cnt']\n",
    "fall = data.loc[data['season'] == 3]['cnt']\n",
    "winter = data.loc[data['season'] == 4]['cnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_stat,p_value = stats.f_oneway(spring,summer,fall,winter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('F-stat:',f_stat)\n",
    "print('p-value:',p_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
