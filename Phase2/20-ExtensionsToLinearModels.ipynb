{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Linear Regression!\n",
    "\n",
    "### Encoding Categorical Variables, Incorporating Interaction and Polynomial Terms, Et Cetera\n",
    "\n",
    "\n",
    "Today's focus is all about translating raw **data** into useful **information** that a model can understand and properly use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But First - A Recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Setting random seed for reproducibility\n",
    "np.random.seed(1000)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New dataset for today! Insurance costs\n",
    "\n",
    "My source: https://www.kaggle.com/mirichoi0218/insurance (they got the idea for cleaning up the original open source data from [Machine Learning with R](https://www.packtpub.com/product/machine-learning-with-r-third-edition/9781788295864))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize relationships between numeric columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize correlations between numeric columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set our X and y\n",
    "# ignore our categorical columns for now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale our data\n",
    "\n",
    "# train on train data\n",
    "\n",
    "# transform both train and test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, let's model!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab predictions for train and test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing our residuals\n",
    "# https://www.scikit-yb.org/en/latest/api/regressor/residuals.html\n",
    "from yellowbrick.regressor import ResidualsPlot\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas to continue improving this model?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Categorical Variables\n",
    "\n",
    "How do we bring in those categorical columns? By **encoding** them - translating the string variables into useful numbers the model can hopefully understand and take meaning from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Common Encoding Method: One Hot Encoding (OHE)\n",
    "\n",
    "Turns categorical columns into binaries, where each option is turned into its own column.\n",
    "\n",
    "For our data: we have a column called `region`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore our region column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With OHE, the result will either be three or four new columns: `is_southeast`, `is_northwest`, `is_southwest`, `is_northeast`\n",
    "\n",
    "For the head of this data:\n",
    "\n",
    "| `is_southeast` | `is_northwest` | `is_southwest` | `is_northeast` |\n",
    "| -------------- | -------------- | -------------- | -------------- | \n",
    "| 0 | 0 | 1 | 0 |\n",
    "| 1 | 0 | 0 | 0 |\n",
    "| 1 | 0 | 0 | 0 |\n",
    "| 0 | 1 | 0 | 0 |\n",
    "| 0 | 1 | 0 | 0 |\n",
    "\n",
    "Why could it be three? We often drop the first column, and allow the model to capture that value by having zeros in all other columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Pandas' `get_dummies()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode variables\n",
    "df_ohe = None\n",
    "\n",
    "print(df_ohe.shape)\n",
    "df_ohe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With `sklearn`'s One Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an encoder object. This will help us to convert\n",
    "# categorical variables to new columns\n",
    "encoder = None\n",
    "\n",
    "# Create an columntransformer object.\n",
    "# This will help us to merge transformed columns\n",
    "# with the rest of the dataset.\n",
    "\n",
    "ct = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can display as a dataframe like so\n",
    "pd.DataFrame(X, columns= ct.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale our data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# train on train data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# transform both train and test data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, let's model!\n",
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# grab predictions for train and test set\n",
    "train_preds = lr.predict(X_train_scaled)\n",
    "test_preds = lr.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "print(f\"Train R2 Score: {r2_score(y_train, train_preds):.3f}\")\n",
    "print(f\"Test R2 Score: {r2_score(y_test, test_preds):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing our residuals\n",
    "visualizer = ResidualsPlot(lr)\n",
    "\n",
    "visualizer.fit(X_train_scaled, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test_scaled, y_test)  # Evaluate the model on the test data\n",
    "visualizer.show()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Pros and Cons of OHE:\n",
    "\n",
    "Pros:\n",
    "\n",
    "- Simple to understand\n",
    "- Easy to implement\n",
    "\n",
    "Cons:\n",
    "\n",
    "- If the categorical column has many options, or there are a lot of categorical columns, you can add _a lot_ more columns - **curse of dimensionality**\n",
    "- Resulting columns are very sparse (mostly zeros)\n",
    "- Resulting columns are directly related (multicollinear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Encoding Methods?\n",
    "\n",
    "Certainly there are other ways to turn a categorical column into numeric data that a model can understand.\n",
    "\n",
    "Some Examples:\n",
    "\n",
    "- [Label/Ordinal Encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html)\n",
    "- [Frequency Encoding](https://contrib.scikit-learn.org/category_encoders/count.html) (just a count encoder with `normalize=True` to turn into a frequency percentage)\n",
    "- [Target Encoding](https://contrib.scikit-learn.org/category_encoders/targetencoder.html) (or, relatedly, [Leave-One-Out Encoding](https://contrib.scikit-learn.org/category_encoders/leaveoneout.html) or [Weight of Evidence Encoding](https://contrib.scikit-learn.org/category_encoders/woe.html))\n",
    "\n",
    "Useful links:\n",
    "\n",
    "- [Category Encoders](https://contrib.scikit-learn.org/category_encoders/index.html) - library of sklearn-style encoders that implement more encoding methods than those actually packaged in Sklearn\n",
    "- [Sklearn's Preprocessing Section](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing) - user guide section on preprocessing (includes scalers and transformers as well as encoders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Terms - Simple Linear Regression\n",
    "\n",
    "Demonstrating this on a toy example, with a single x variable predicting y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 150 samples from uniform distribution between -2pi and 2pi\n",
    "\n",
    "x = np.random.uniform(-2*np.pi, 2*np.pi, 150)\n",
    "\n",
    "# Creating target (y) - so we know the true relationship between x and y\n",
    "# But - adding some noise (error) with 'np.random'\n",
    "\n",
    "y = np.sin(x) + np.random.normal(loc=0, scale=0.4, size=len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize it\n",
    "plt.scatter(x, y)\n",
    "\n",
    "plt.ylabel('$\\sin(x)$ plus noise')\n",
    "plt.xlabel('x values are randomly chosen from $[-2\\pi, 2\\pi]$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a linear model\n",
    "lr = LinearRegression()\n",
    "lr.fit(x.reshape(-1, 1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing the predicted values\n",
    "y_pred = lr.predict(x.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring our model\n",
    "print(f\"R2 Score: {r2_score(y, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize it\n",
    "plt.scatter(x, y) # original data\n",
    "\n",
    "plt.plot(x, y_pred, c='red') # predicted values\n",
    "\n",
    "plt.ylabel('$\\sin(x)$ + noise')\n",
    "plt.xlabel('x values randomly chosen between $-2\\pi$ and $2\\pi$')\n",
    "plt.title(\"Simple Linear Regression\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this a good model? Well - of course not. It's definitely **underfit** - it is not complex enough to accurately capture the pattern and predict the target.\n",
    "\n",
    "Let's try again, but now with polynomials!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this, we'll need some helper functions\n",
    "# Shoutout to Andy for sending me these\n",
    "\n",
    "def create_poly_dataset(x, degree):\n",
    "    \"\"\"\n",
    "    returning dataset with the given polynomial degree\n",
    "    \"\"\"\n",
    "    # Instantiate the PolynomialFeatures object with given 'degree'\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Now transform data to create higher order features\n",
    "    new_data = poly.fit_transform(x.reshape(-1, 1))\n",
    "    return new_data\n",
    "\n",
    "def fit_linear_model(data, y):\n",
    "    \"\"\"\n",
    "    fitting a linear model and printing model details\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "    if data.ndim == 1:\n",
    "        data = data.reshape(-1, 1)\n",
    "\n",
    "    lr = LinearRegression(fit_intercept=False)\n",
    "    lr.fit(data, y)\n",
    "    print(\"-\"*13)\n",
    "    print(\"Coefficients: \", lr.coef_)\n",
    "    y_pred = lr.predict(data)\n",
    "    print(f\"R-Squared: {lr.score(data, y):.3f}\")\n",
    "    return lr\n",
    "\n",
    "def plot_predict(x, y, model):\n",
    "    \"\"\"\n",
    "    plotting predictions against true values\n",
    "    \"\"\"\n",
    "    plt.scatter(x, y, label='true')\n",
    "    x_pred = np.linspace(x.min(), x.max(), 100)\n",
    "    \n",
    "    # visualize beyond this x range by uncommenting below:\n",
    "#     extra = x.ptp() * .2\n",
    "#     x_pred = np.linspace(x.min() - extra, x.max() + extra, 100)\n",
    "\n",
    "    plt.plot(x_pred, model.predict(create_poly_dataset(x_pred, len(model.coef_)-1)),\n",
    "             label='predicted', c='red')\n",
    "\n",
    "    if len(model.coef_) == 1:\n",
    "        plt.title(f\"{len(model.coef_) - 1} Polynomial Terms \\n (no slope)\")\n",
    "    elif (len(model.coef_) - 1) == 1:\n",
    "        plt.title(f\"{len(model.coef_) - 1} Polynomial Term\")\n",
    "    else:\n",
    "        plt.title(f\"{len(model.coef_) - 1} Polynomial Terms\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualizing an assortment of polynomial degrees\n",
    "# can visualize each sequential polynomial with `range(n)`\n",
    "for i in [0, 1, 2, 3, 5, 7, 9, 13, 18]:\n",
    "    xi = create_poly_dataset(x, i)\n",
    "    plot_predict(x, y, fit_linear_model(xi, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate: which of these is the best?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate: so what?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When do we need interaction terms? And how do we check for them?\n",
    "\n",
    "Well, first things first - what interactions do _you_ think would make sense? That's the easiest way to incorporate interaction terms - use domain knowledge to think through what usefully could be combined into an interaction.\n",
    "\n",
    "As for how to check if something might be better captured as an interaction..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the target back onto our OHE df\n",
    "df_ohe['target'] = df['charges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of no interaction term...\n",
    "sns.lmplot(x='age', y='target', hue='smoker_yes', data=df_ohe, scatter=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do I know these two variables, `age` and `smoker_yes`, aren't interacting? \n",
    "\n",
    "- Look at the slopes - parallel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's look at something else...\n",
    "sns.lmplot(x='bmi', y='target', hue='smoker_yes', data=df_ohe, scatter=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Interaction and Polynomials in Sklearn\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_ohe.drop(columns = 'target')\n",
    "y = df_ohe['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first do Polynomials, to the 3rd degree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=3, interaction_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_poly = poly.transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_poly = pd.DataFrame(X_train_poly, columns = poly.get_feature_names())\n",
    "X_train_poly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_poly.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_poly.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still need to scale\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train_poly)\n",
    "\n",
    "X_train_poly_sc = scaler.transform(X_train_poly)\n",
    "X_test_poly_sc = scaler.transform(X_test_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(X_train_poly_sc, y_train)\n",
    "\n",
    "X_train_poly_preds = lr.predict(X_train_poly_sc)\n",
    "X_test_poly_preds = lr.predict(X_test_poly_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "print(f\"Train R2 Score: {r2_score(y_train, X_train_poly_preds):.3f}\")\n",
    "print(f\"Test R2 Score: {r2_score(y_test, X_test_poly_preds):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "\n",
    "interactions.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ints = interactions.transform(X_train)\n",
    "X_test_ints = interactions.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ints = pd.DataFrame(X_train_ints, columns = interactions.get_feature_names())\n",
    "X_train_ints.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still need to scale\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train_ints)\n",
    "\n",
    "X_train_ints_sc = scaler.transform(X_train_ints)\n",
    "X_test_ints_sc = scaler.transform(X_test_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(X_train_ints_sc, y_train)\n",
    "\n",
    "X_train_ints_preds = lr.predict(X_train_ints_sc)\n",
    "X_test_ints_preds = lr.predict(X_test_ints_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "print(f\"Train R2 Score: {r2_score(y_train, X_train_ints_preds):.3f}\")\n",
    "print(f\"Test R2 Score: {r2_score(y_test, X_test_ints_preds):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate: What do you think? Is this blanket way of approaching polynomial or interaction terms useful?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection and Feature Importances...\n",
    "\n",
    "Not much time to do this, but:\n",
    "\n",
    "- Lasso Regression (L1 regularization)\n",
    "- Recursive Feature Elimination\n",
    "- Forward Stepwise Selection\n",
    "\n",
    "Can always check out the python library [`eli5`](https://eli5.readthedocs.io/en/latest/index.html) (yes, Explain Like I'm 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources:\n",
    "\n",
    "[Feature Engineering and Selection: A Practical Approach for Predictive Models](https://bookdown.org/max/FES/) (computing done in R, but book focuses mostly on discussing the hows and whys rather than focusing on implementation)\n",
    "\n",
    "- Their chapter on [Encoding Categorical Predictors](https://bookdown.org/max/FES/encoding-categorical-predictors.html)\n",
    "- And their chapter on [Detecting Interaction Effects](https://bookdown.org/max/FES/detecting-interaction-effects.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
